{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overtraining 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is bases on Adrian Bevan's \"NN_parabola - This is a Multilayer Perceptron (MLP) example using Keras\" jupyter notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mGenerating the parabola data set\u001b[0m\n",
      "Have generated the following data:\n",
      "\tN(test)  =  1000\n",
      "\tN(train) =  100\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "Ntrain = 100\n",
    "Ntest  = 1000\n",
    "xmin   =-10\n",
    "xmax   = 10\n",
    "Noise  = 0.1\n",
    "print(\"\\033[92mGenerating the parabola data set\\033[0m\")\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_test  = []\n",
    "Y_test  = []\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "def sim_parabola(xmin, xmax, Noise):\n",
    "    \"\"\"\n",
    "    Function to simulate a random data point for a parabola\n",
    "    \"\"\"\n",
    "    x = random.random()*(xmax-xmin)+xmin\n",
    "    y = x*x*(1+random.random()*Noise)\n",
    "    \n",
    "    return x, y\n",
    "#--------------------------------------------------------------------\n",
    "  \n",
    "for i in range( Ntrain ):\n",
    "  x,y = sim_parabola(xmin, xmax, Noise)\n",
    "  X_train.append(x)\n",
    "  Y_train.append(y)\n",
    "\n",
    "for i in range( Ntest ):\n",
    "  x,y = sim_parabola(xmin, xmax, Noise)\n",
    "  X_test.append(x)\n",
    "  Y_test.append(y)\n",
    "\n",
    "# convert to nparrays\n",
    "x_test  = np.array(X_test)\n",
    "y_test  = np.array(Y_test)\n",
    "x_train = np.array(X_train)\n",
    "y_train = np.array(Y_train)\n",
    "\n",
    "print(\"Have generated the following data:\")\n",
    "print(\"\\tN(test)  = \", len(x_test))\n",
    "print(\"\\tN(train) = \", len(x_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92mWill train a multilayer perceptron using some toy data following y = x^2\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Input data MNIST\n",
      "Leaky relu parameter =  0.1\n",
      "ValidationSplit      =  0.2\n",
      "BatchSize            =  32\n",
      "Nepochs              =  1000\n",
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1000\n",
      "80/80 [==============================] - 1s 17ms/sample - loss: 2481.1690 - val_loss: 2482.3020\n",
      "Epoch 2/1000\n",
      "80/80 [==============================] - 0s 498us/sample - loss: 2378.6096 - val_loss: 2367.8369\n",
      "Epoch 3/1000\n",
      "80/80 [==============================] - 0s 307us/sample - loss: 2256.7958 - val_loss: 2222.7563\n",
      "Epoch 4/1000\n",
      "80/80 [==============================] - 0s 354us/sample - loss: 2113.7301 - val_loss: 2038.5300\n",
      "Epoch 5/1000\n",
      "80/80 [==============================] - 0s 353us/sample - loss: 1914.2679 - val_loss: 1813.6960\n",
      "Epoch 6/1000\n",
      "80/80 [==============================] - 0s 490us/sample - loss: 1684.2283 - val_loss: 1543.6418\n",
      "Epoch 7/1000\n",
      "80/80 [==============================] - 0s 418us/sample - loss: 1429.0537 - val_loss: 1239.9841\n",
      "Epoch 8/1000\n",
      "80/80 [==============================] - 0s 347us/sample - loss: 1105.1521 - val_loss: 920.5505\n",
      "Epoch 9/1000\n",
      "80/80 [==============================] - 0s 320us/sample - loss: 822.7743 - val_loss: 611.3866\n",
      "Epoch 10/1000\n",
      "80/80 [==============================] - 0s 353us/sample - loss: 535.2322 - val_loss: 361.0962\n",
      "Epoch 11/1000\n",
      "80/80 [==============================] - 0s 404us/sample - loss: 325.0357 - val_loss: 213.4387\n",
      "Epoch 12/1000\n",
      "80/80 [==============================] - 0s 376us/sample - loss: 227.6333 - val_loss: 169.7035\n",
      "Epoch 13/1000\n",
      "80/80 [==============================] - 0s 362us/sample - loss: 216.2561 - val_loss: 175.5291\n",
      "Epoch 14/1000\n",
      "80/80 [==============================] - 0s 392us/sample - loss: 245.4370 - val_loss: 178.0634\n",
      "Epoch 15/1000\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 242.1046 - val_loss: 163.8352\n",
      "Epoch 16/1000\n",
      "80/80 [==============================] - 0s 297us/sample - loss: 213.1691 - val_loss: 152.0155\n",
      "Epoch 17/1000\n",
      "80/80 [==============================] - 0s 363us/sample - loss: 174.4537 - val_loss: 154.7919\n",
      "Epoch 18/1000\n",
      "80/80 [==============================] - 0s 454us/sample - loss: 162.5772 - val_loss: 165.2325\n",
      "Epoch 19/1000\n",
      "80/80 [==============================] - 0s 351us/sample - loss: 161.8922 - val_loss: 173.5753\n",
      "Epoch 20/1000\n",
      "80/80 [==============================] - 0s 375us/sample - loss: 162.4941 - val_loss: 169.7616\n",
      "Epoch 21/1000\n",
      "80/80 [==============================] - 0s 451us/sample - loss: 160.6125 - val_loss: 158.5015\n",
      "Epoch 22/1000\n",
      "80/80 [==============================] - 0s 539us/sample - loss: 155.8057 - val_loss: 147.3366\n",
      "Epoch 23/1000\n",
      "80/80 [==============================] - 0s 327us/sample - loss: 153.0796 - val_loss: 137.5369\n",
      "Epoch 24/1000\n",
      "80/80 [==============================] - 0s 320us/sample - loss: 149.6393 - val_loss: 131.2912\n",
      "Epoch 25/1000\n",
      "80/80 [==============================] - 0s 352us/sample - loss: 148.8659 - val_loss: 127.8434\n",
      "Epoch 26/1000\n",
      "80/80 [==============================] - 0s 429us/sample - loss: 148.4132 - val_loss: 126.5269\n",
      "Epoch 27/1000\n",
      "80/80 [==============================] - 0s 384us/sample - loss: 146.7009 - val_loss: 127.3821\n",
      "Epoch 28/1000\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 145.2392 - val_loss: 130.8190\n",
      "Epoch 29/1000\n",
      "80/80 [==============================] - 0s 443us/sample - loss: 143.5735 - val_loss: 133.7657\n",
      "Epoch 30/1000\n",
      "80/80 [==============================] - 0s 417us/sample - loss: 143.1006 - val_loss: 135.8385\n",
      "Epoch 31/1000\n",
      "80/80 [==============================] - 0s 301us/sample - loss: 142.4518 - val_loss: 135.8504\n",
      "Epoch 32/1000\n",
      "80/80 [==============================] - 0s 311us/sample - loss: 141.5176 - val_loss: 133.0536\n",
      "Epoch 33/1000\n",
      "80/80 [==============================] - 0s 355us/sample - loss: 139.4123 - val_loss: 127.5808\n",
      "Epoch 34/1000\n",
      "80/80 [==============================] - 0s 303us/sample - loss: 137.8702 - val_loss: 120.4196\n",
      "Epoch 35/1000\n",
      "80/80 [==============================] - 0s 345us/sample - loss: 135.9009 - val_loss: 115.0218\n",
      "Epoch 36/1000\n",
      "80/80 [==============================] - 0s 324us/sample - loss: 137.3855 - val_loss: 111.2998\n",
      "Epoch 37/1000\n",
      "80/80 [==============================] - 0s 343us/sample - loss: 138.0500 - val_loss: 111.1253\n",
      "Epoch 38/1000\n",
      "80/80 [==============================] - 0s 517us/sample - loss: 134.3239 - val_loss: 110.2846\n",
      "Epoch 39/1000\n",
      "80/80 [==============================] - 0s 358us/sample - loss: 132.6894 - val_loss: 110.5386\n",
      "Epoch 40/1000\n",
      "80/80 [==============================] - 0s 341us/sample - loss: 132.1013 - val_loss: 110.2362\n",
      "Epoch 41/1000\n",
      "80/80 [==============================] - 0s 316us/sample - loss: 129.7763 - val_loss: 110.2045\n",
      "Epoch 42/1000\n",
      "80/80 [==============================] - 0s 298us/sample - loss: 128.2446 - val_loss: 109.6309\n",
      "Epoch 43/1000\n",
      "80/80 [==============================] - 0s 331us/sample - loss: 126.9559 - val_loss: 110.0237\n",
      "Epoch 44/1000\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 126.4296 - val_loss: 109.3076\n",
      "Epoch 45/1000\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 125.1675 - val_loss: 108.5106\n",
      "Epoch 46/1000\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 123.7783 - val_loss: 106.6598\n",
      "Epoch 47/1000\n",
      "80/80 [==============================] - 0s 357us/sample - loss: 122.5072 - val_loss: 103.7183\n",
      "Epoch 48/1000\n",
      "80/80 [==============================] - 0s 313us/sample - loss: 122.7413 - val_loss: 98.6460\n",
      "Epoch 49/1000\n",
      "80/80 [==============================] - 0s 316us/sample - loss: 119.6821 - val_loss: 97.3310\n",
      "Epoch 50/1000\n",
      "80/80 [==============================] - 0s 373us/sample - loss: 118.6842 - val_loss: 96.6442\n",
      "Epoch 51/1000\n",
      "80/80 [==============================] - 0s 374us/sample - loss: 117.3028 - val_loss: 93.8004\n",
      "Epoch 52/1000\n",
      "80/80 [==============================] - 0s 402us/sample - loss: 117.4058 - val_loss: 91.3343\n",
      "Epoch 53/1000\n",
      "80/80 [==============================] - 0s 390us/sample - loss: 114.6548 - val_loss: 91.5032\n",
      "Epoch 54/1000\n",
      "80/80 [==============================] - 0s 545us/sample - loss: 114.9885 - val_loss: 92.8353\n",
      "Epoch 55/1000\n",
      "80/80 [==============================] - 0s 462us/sample - loss: 113.1593 - val_loss: 92.9418\n",
      "Epoch 56/1000\n",
      "80/80 [==============================] - 0s 340us/sample - loss: 111.6940 - val_loss: 89.6712\n",
      "Epoch 57/1000\n",
      "80/80 [==============================] - 0s 428us/sample - loss: 109.8228 - val_loss: 87.8210\n",
      "Epoch 58/1000\n",
      "80/80 [==============================] - 0s 289us/sample - loss: 108.8860 - val_loss: 85.5058\n",
      "Epoch 59/1000\n",
      "80/80 [==============================] - 0s 394us/sample - loss: 107.5695 - val_loss: 84.9260\n",
      "Epoch 60/1000\n",
      "80/80 [==============================] - 0s 371us/sample - loss: 106.9839 - val_loss: 82.3586\n",
      "Epoch 61/1000\n",
      "80/80 [==============================] - 0s 321us/sample - loss: 104.1311 - val_loss: 77.1463\n",
      "Epoch 62/1000\n",
      "80/80 [==============================] - 0s 281us/sample - loss: 104.7459 - val_loss: 73.2578\n",
      "Epoch 63/1000\n",
      "80/80 [==============================] - 0s 288us/sample - loss: 103.5462 - val_loss: 73.6393\n",
      "Epoch 64/1000\n",
      "80/80 [==============================] - 0s 320us/sample - loss: 101.4063 - val_loss: 71.3878\n",
      "Epoch 65/1000\n",
      "80/80 [==============================] - 0s 289us/sample - loss: 99.4858 - val_loss: 71.2514\n",
      "Epoch 66/1000\n",
      "80/80 [==============================] - 0s 289us/sample - loss: 98.3985 - val_loss: 72.4567\n",
      "Epoch 67/1000\n",
      "80/80 [==============================] - 0s 324us/sample - loss: 96.3392 - val_loss: 71.1749\n",
      "Epoch 68/1000\n",
      "80/80 [==============================] - 0s 373us/sample - loss: 94.9022 - val_loss: 69.0001\n",
      "Epoch 69/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 99.42 - 0s 305us/sample - loss: 94.1102 - val_loss: 67.1149\n",
      "Epoch 70/1000\n",
      "80/80 [==============================] - 0s 451us/sample - loss: 92.1335 - val_loss: 65.8926\n",
      "Epoch 71/1000\n",
      "80/80 [==============================] - 0s 453us/sample - loss: 91.0019 - val_loss: 64.1030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/1000\n",
      "80/80 [==============================] - 0s 345us/sample - loss: 90.1518 - val_loss: 63.1862\n",
      "Epoch 73/1000\n",
      "80/80 [==============================] - 0s 378us/sample - loss: 88.5020 - val_loss: 64.7012\n",
      "Epoch 74/1000\n",
      "80/80 [==============================] - 0s 439us/sample - loss: 86.9008 - val_loss: 62.8415\n",
      "Epoch 75/1000\n",
      "80/80 [==============================] - 0s 482us/sample - loss: 85.6041 - val_loss: 58.9245\n",
      "Epoch 76/1000\n",
      "80/80 [==============================] - 0s 476us/sample - loss: 82.6270 - val_loss: 53.6809\n",
      "Epoch 77/1000\n",
      "80/80 [==============================] - 0s 543us/sample - loss: 84.3809 - val_loss: 51.1458\n",
      "Epoch 78/1000\n",
      "80/80 [==============================] - 0s 473us/sample - loss: 83.5017 - val_loss: 50.7740\n",
      "Epoch 79/1000\n",
      "80/80 [==============================] - 0s 324us/sample - loss: 82.5077 - val_loss: 52.6861\n",
      "Epoch 80/1000\n",
      "80/80 [==============================] - 0s 424us/sample - loss: 79.0586 - val_loss: 53.2497\n",
      "Epoch 81/1000\n",
      "80/80 [==============================] - 0s 399us/sample - loss: 76.8860 - val_loss: 51.6319\n",
      "Epoch 82/1000\n",
      "80/80 [==============================] - 0s 394us/sample - loss: 75.2960 - val_loss: 49.9858\n",
      "Epoch 83/1000\n",
      "80/80 [==============================] - 0s 396us/sample - loss: 73.6552 - val_loss: 48.2877\n",
      "Epoch 84/1000\n",
      "80/80 [==============================] - 0s 652us/sample - loss: 72.5880 - val_loss: 46.9397\n",
      "Epoch 85/1000\n",
      "80/80 [==============================] - 0s 404us/sample - loss: 71.1027 - val_loss: 43.2338\n",
      "Epoch 86/1000\n",
      "80/80 [==============================] - 0s 466us/sample - loss: 69.4688 - val_loss: 41.9173\n",
      "Epoch 87/1000\n",
      "80/80 [==============================] - 0s 381us/sample - loss: 68.2828 - val_loss: 41.3145\n",
      "Epoch 88/1000\n",
      "80/80 [==============================] - 0s 316us/sample - loss: 66.7964 - val_loss: 39.6663\n",
      "Epoch 89/1000\n",
      "80/80 [==============================] - 0s 377us/sample - loss: 65.6637 - val_loss: 39.7628\n",
      "Epoch 90/1000\n",
      "80/80 [==============================] - 0s 437us/sample - loss: 63.6276 - val_loss: 40.0267\n",
      "Epoch 91/1000\n",
      "80/80 [==============================] - 0s 264us/sample - loss: 63.0342 - val_loss: 39.3483\n",
      "Epoch 92/1000\n",
      "80/80 [==============================] - 0s 388us/sample - loss: 60.6582 - val_loss: 35.1412\n",
      "Epoch 93/1000\n",
      "80/80 [==============================] - 0s 331us/sample - loss: 59.4042 - val_loss: 32.1656\n",
      "Epoch 94/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 38.53 - 0s 512us/sample - loss: 58.2847 - val_loss: 32.0023\n",
      "Epoch 95/1000\n",
      "80/80 [==============================] - 0s 502us/sample - loss: 57.5537 - val_loss: 33.7278\n",
      "Epoch 96/1000\n",
      "80/80 [==============================] - 0s 392us/sample - loss: 55.7975 - val_loss: 34.3405\n",
      "Epoch 97/1000\n",
      "80/80 [==============================] - 0s 520us/sample - loss: 54.0901 - val_loss: 33.5311\n",
      "Epoch 98/1000\n",
      "80/80 [==============================] - 0s 545us/sample - loss: 53.4067 - val_loss: 32.9461\n",
      "Epoch 99/1000\n",
      "80/80 [==============================] - 0s 479us/sample - loss: 51.7154 - val_loss: 29.9708\n",
      "Epoch 100/1000\n",
      "80/80 [==============================] - 0s 362us/sample - loss: 49.6429 - val_loss: 26.8347\n",
      "Epoch 101/1000\n",
      "80/80 [==============================] - 0s 481us/sample - loss: 49.8833 - val_loss: 24.3100\n",
      "Epoch 102/1000\n",
      "80/80 [==============================] - 0s 367us/sample - loss: 48.5353 - val_loss: 24.0374\n",
      "Epoch 103/1000\n",
      "80/80 [==============================] - 0s 319us/sample - loss: 47.4625 - val_loss: 25.4992\n",
      "Epoch 104/1000\n",
      "80/80 [==============================] - 0s 271us/sample - loss: 44.8529 - val_loss: 23.8739\n",
      "Epoch 105/1000\n",
      "80/80 [==============================] - 0s 447us/sample - loss: 43.7901 - val_loss: 23.2557\n",
      "Epoch 106/1000\n",
      "80/80 [==============================] - 0s 413us/sample - loss: 43.5667 - val_loss: 21.8895\n",
      "Epoch 107/1000\n",
      "80/80 [==============================] - 0s 369us/sample - loss: 41.3429 - val_loss: 21.8363\n",
      "Epoch 108/1000\n",
      "80/80 [==============================] - 0s 428us/sample - loss: 40.0304 - val_loss: 22.6781\n",
      "Epoch 109/1000\n",
      "80/80 [==============================] - 0s 278us/sample - loss: 39.2088 - val_loss: 24.2931\n",
      "Epoch 110/1000\n",
      "80/80 [==============================] - 0s 405us/sample - loss: 38.7163 - val_loss: 23.1660\n",
      "Epoch 111/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 41.84 - 0s 304us/sample - loss: 36.7216 - val_loss: 19.9362\n",
      "Epoch 112/1000\n",
      "80/80 [==============================] - 0s 367us/sample - loss: 35.6379 - val_loss: 18.0069\n",
      "Epoch 113/1000\n",
      "80/80 [==============================] - 0s 474us/sample - loss: 35.4972 - val_loss: 16.8958\n",
      "Epoch 114/1000\n",
      "80/80 [==============================] - 0s 486us/sample - loss: 34.2362 - val_loss: 16.4509\n",
      "Epoch 115/1000\n",
      "80/80 [==============================] - 0s 360us/sample - loss: 32.3693 - val_loss: 17.6752\n",
      "Epoch 116/1000\n",
      "80/80 [==============================] - 0s 532us/sample - loss: 31.5665 - val_loss: 19.4037\n",
      "Epoch 117/1000\n",
      "80/80 [==============================] - 0s 338us/sample - loss: 31.3034 - val_loss: 18.9930\n",
      "Epoch 118/1000\n",
      "80/80 [==============================] - 0s 261us/sample - loss: 30.4904 - val_loss: 16.9607\n",
      "Epoch 119/1000\n",
      "80/80 [==============================] - 0s 436us/sample - loss: 28.7467 - val_loss: 14.2698\n",
      "Epoch 120/1000\n",
      "80/80 [==============================] - 0s 358us/sample - loss: 27.4750 - val_loss: 12.9224\n",
      "Epoch 121/1000\n",
      "80/80 [==============================] - 0s 317us/sample - loss: 27.7369 - val_loss: 12.6833\n",
      "Epoch 122/1000\n",
      "80/80 [==============================] - 0s 322us/sample - loss: 26.1349 - val_loss: 13.9764\n",
      "Epoch 123/1000\n",
      "80/80 [==============================] - 0s 350us/sample - loss: 24.8141 - val_loss: 15.5401\n",
      "Epoch 124/1000\n",
      "80/80 [==============================] - 0s 362us/sample - loss: 25.4871 - val_loss: 16.0280\n",
      "Epoch 125/1000\n",
      "80/80 [==============================] - 0s 270us/sample - loss: 23.9862 - val_loss: 13.2109\n",
      "Epoch 126/1000\n",
      "80/80 [==============================] - 0s 406us/sample - loss: 22.6960 - val_loss: 11.5479\n",
      "Epoch 127/1000\n",
      "80/80 [==============================] - 0s 333us/sample - loss: 23.1392 - val_loss: 10.8316\n",
      "Epoch 128/1000\n",
      "80/80 [==============================] - 0s 332us/sample - loss: 22.2104 - val_loss: 11.3775\n",
      "Epoch 129/1000\n",
      "80/80 [==============================] - 0s 542us/sample - loss: 21.9127 - val_loss: 13.3145\n",
      "Epoch 130/1000\n",
      "80/80 [==============================] - 0s 270us/sample - loss: 21.3533 - val_loss: 12.9341\n",
      "Epoch 131/1000\n",
      "80/80 [==============================] - 0s 284us/sample - loss: 20.1104 - val_loss: 11.2591\n",
      "Epoch 132/1000\n",
      "80/80 [==============================] - 0s 518us/sample - loss: 19.4356 - val_loss: 9.9358\n",
      "Epoch 133/1000\n",
      "80/80 [==============================] - 0s 419us/sample - loss: 18.6710 - val_loss: 9.8462\n",
      "Epoch 134/1000\n",
      "80/80 [==============================] - 0s 410us/sample - loss: 18.1092 - val_loss: 10.2428\n",
      "Epoch 135/1000\n",
      "80/80 [==============================] - 0s 369us/sample - loss: 17.3968 - val_loss: 10.0989\n",
      "Epoch 136/1000\n",
      "80/80 [==============================] - 0s 280us/sample - loss: 17.0026 - val_loss: 9.7386\n",
      "Epoch 137/1000\n",
      "80/80 [==============================] - 0s 347us/sample - loss: 16.6481 - val_loss: 9.3283\n",
      "Epoch 138/1000\n",
      "80/80 [==============================] - 0s 367us/sample - loss: 16.5819 - val_loss: 9.2675\n",
      "Epoch 139/1000\n",
      "80/80 [==============================] - 0s 372us/sample - loss: 15.6714 - val_loss: 9.5157\n",
      "Epoch 140/1000\n",
      "80/80 [==============================] - 0s 437us/sample - loss: 15.2092 - val_loss: 9.0435\n",
      "Epoch 141/1000\n",
      "80/80 [==============================] - 0s 353us/sample - loss: 15.0588 - val_loss: 8.7095\n",
      "Epoch 142/1000\n",
      "80/80 [==============================] - 0s 301us/sample - loss: 14.4605 - val_loss: 8.7856\n",
      "Epoch 143/1000\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 13.8965 - val_loss: 9.1367\n",
      "Epoch 144/1000\n",
      "80/80 [==============================] - 0s 364us/sample - loss: 13.5194 - val_loss: 8.7929\n",
      "Epoch 145/1000\n",
      "80/80 [==============================] - 0s 433us/sample - loss: 13.0139 - val_loss: 8.5067\n",
      "Epoch 146/1000\n",
      "80/80 [==============================] - 0s 378us/sample - loss: 12.9424 - val_loss: 8.3791\n",
      "Epoch 147/1000\n",
      "80/80 [==============================] - 0s 301us/sample - loss: 12.5063 - val_loss: 8.4738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/1000\n",
      "80/80 [==============================] - 0s 371us/sample - loss: 12.0781 - val_loss: 8.3179\n",
      "Epoch 149/1000\n",
      "80/80 [==============================] - 0s 335us/sample - loss: 11.6783 - val_loss: 8.1821\n",
      "Epoch 150/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 9.388 - 0s 315us/sample - loss: 11.7510 - val_loss: 8.1656\n",
      "Epoch 151/1000\n",
      "80/80 [==============================] - 0s 356us/sample - loss: 10.8357 - val_loss: 8.5207\n",
      "Epoch 152/1000\n",
      "80/80 [==============================] - 0s 435us/sample - loss: 11.5709 - val_loss: 8.8535\n",
      "Epoch 153/1000\n",
      "80/80 [==============================] - 0s 455us/sample - loss: 10.7010 - val_loss: 7.9387\n",
      "Epoch 154/1000\n",
      "80/80 [==============================] - 0s 365us/sample - loss: 11.0484 - val_loss: 8.0157\n",
      "Epoch 155/1000\n",
      "80/80 [==============================] - 0s 361us/sample - loss: 10.0664 - val_loss: 8.5114\n",
      "Epoch 156/1000\n",
      "80/80 [==============================] - 0s 513us/sample - loss: 10.2162 - val_loss: 8.6941\n",
      "Epoch 157/1000\n",
      "80/80 [==============================] - 0s 426us/sample - loss: 9.8801 - val_loss: 8.6069\n",
      "Epoch 158/1000\n",
      "80/80 [==============================] - 0s 494us/sample - loss: 9.8856 - val_loss: 7.7972\n",
      "Epoch 159/1000\n",
      "80/80 [==============================] - 0s 659us/sample - loss: 9.1926 - val_loss: 7.8978\n",
      "Epoch 160/1000\n",
      "80/80 [==============================] - 0s 570us/sample - loss: 9.1577 - val_loss: 7.7091\n",
      "Epoch 161/1000\n",
      "80/80 [==============================] - 0s 376us/sample - loss: 8.5218 - val_loss: 7.8120\n",
      "Epoch 162/1000\n",
      "80/80 [==============================] - 0s 350us/sample - loss: 8.8798 - val_loss: 8.7921\n",
      "Epoch 163/1000\n",
      "80/80 [==============================] - 0s 384us/sample - loss: 8.4612 - val_loss: 8.2503\n",
      "Epoch 164/1000\n",
      "80/80 [==============================] - 0s 424us/sample - loss: 8.6616 - val_loss: 7.9422\n",
      "Epoch 165/1000\n",
      "80/80 [==============================] - 0s 392us/sample - loss: 8.5484 - val_loss: 7.5698\n",
      "Epoch 166/1000\n",
      "80/80 [==============================] - 0s 453us/sample - loss: 7.6913 - val_loss: 7.9888\n",
      "Epoch 167/1000\n",
      "80/80 [==============================] - 0s 508us/sample - loss: 8.3745 - val_loss: 8.3073\n",
      "Epoch 168/1000\n",
      "80/80 [==============================] - 0s 460us/sample - loss: 7.8690 - val_loss: 8.2634\n",
      "Epoch 169/1000\n",
      "80/80 [==============================] - 0s 466us/sample - loss: 7.8121 - val_loss: 7.6237\n",
      "Epoch 170/1000\n",
      "80/80 [==============================] - 0s 468us/sample - loss: 7.4578 - val_loss: 7.6341\n",
      "Epoch 171/1000\n",
      "80/80 [==============================] - 0s 423us/sample - loss: 7.1078 - val_loss: 8.0142\n",
      "Epoch 172/1000\n",
      "80/80 [==============================] - 0s 421us/sample - loss: 7.1276 - val_loss: 7.9263\n",
      "Epoch 173/1000\n",
      "80/80 [==============================] - 0s 655us/sample - loss: 7.1968 - val_loss: 7.5701\n",
      "Epoch 174/1000\n",
      "80/80 [==============================] - 0s 366us/sample - loss: 6.8886 - val_loss: 7.0422\n",
      "Epoch 175/1000\n",
      "80/80 [==============================] - 0s 344us/sample - loss: 7.0449 - val_loss: 7.3532\n",
      "Epoch 176/1000\n",
      "80/80 [==============================] - 0s 416us/sample - loss: 6.9025 - val_loss: 9.2551\n",
      "Epoch 177/1000\n",
      "80/80 [==============================] - 0s 384us/sample - loss: 6.9187 - val_loss: 8.5257\n",
      "Epoch 178/1000\n",
      "80/80 [==============================] - 0s 383us/sample - loss: 6.6407 - val_loss: 7.1818\n",
      "Epoch 179/1000\n",
      "80/80 [==============================] - 0s 350us/sample - loss: 6.7878 - val_loss: 6.8498\n",
      "Epoch 180/1000\n",
      "80/80 [==============================] - 0s 276us/sample - loss: 6.3097 - val_loss: 8.1086\n",
      "Epoch 181/1000\n",
      "80/80 [==============================] - 0s 517us/sample - loss: 6.6061 - val_loss: 9.8467\n",
      "Epoch 182/1000\n",
      "80/80 [==============================] - 0s 444us/sample - loss: 6.8601 - val_loss: 8.5702\n",
      "Epoch 183/1000\n",
      "80/80 [==============================] - 0s 373us/sample - loss: 6.2105 - val_loss: 7.1985\n",
      "Epoch 184/1000\n",
      "80/80 [==============================] - 0s 392us/sample - loss: 6.3056 - val_loss: 7.4296\n",
      "Epoch 185/1000\n",
      "80/80 [==============================] - 0s 342us/sample - loss: 6.1669 - val_loss: 7.3670\n",
      "Epoch 186/1000\n",
      "80/80 [==============================] - 0s 413us/sample - loss: 6.1788 - val_loss: 7.8985\n",
      "Epoch 187/1000\n",
      "80/80 [==============================] - 0s 429us/sample - loss: 6.0941 - val_loss: 7.8110\n",
      "Epoch 188/1000\n",
      "80/80 [==============================] - 0s 585us/sample - loss: 5.7120 - val_loss: 7.4465\n",
      "Epoch 189/1000\n",
      "80/80 [==============================] - 0s 345us/sample - loss: 5.8123 - val_loss: 6.8838\n",
      "Epoch 190/1000\n",
      "80/80 [==============================] - 0s 459us/sample - loss: 5.6870 - val_loss: 7.0185\n",
      "Epoch 191/1000\n",
      "80/80 [==============================] - 0s 359us/sample - loss: 5.5727 - val_loss: 7.8124\n",
      "Epoch 192/1000\n",
      "80/80 [==============================] - 0s 306us/sample - loss: 5.5658 - val_loss: 8.0044\n",
      "Epoch 193/1000\n",
      "80/80 [==============================] - 0s 371us/sample - loss: 5.4690 - val_loss: 6.7642\n",
      "Epoch 194/1000\n",
      "80/80 [==============================] - 0s 333us/sample - loss: 5.6400 - val_loss: 6.8752\n",
      "Epoch 195/1000\n",
      "80/80 [==============================] - 0s 507us/sample - loss: 5.6020 - val_loss: 8.5843\n",
      "Epoch 196/1000\n",
      "80/80 [==============================] - 0s 478us/sample - loss: 5.7091 - val_loss: 8.6955\n",
      "Epoch 197/1000\n",
      "80/80 [==============================] - 0s 376us/sample - loss: 5.2769 - val_loss: 6.9281\n",
      "Epoch 198/1000\n",
      "80/80 [==============================] - 0s 328us/sample - loss: 5.4259 - val_loss: 6.6596\n",
      "Epoch 199/1000\n",
      "80/80 [==============================] - 0s 466us/sample - loss: 5.4354 - val_loss: 7.7215\n",
      "Epoch 200/1000\n",
      "80/80 [==============================] - 0s 395us/sample - loss: 5.2483 - val_loss: 7.5793\n",
      "Epoch 201/1000\n",
      "80/80 [==============================] - 0s 396us/sample - loss: 5.2033 - val_loss: 7.3110\n",
      "Epoch 202/1000\n",
      "80/80 [==============================] - 0s 462us/sample - loss: 5.1554 - val_loss: 7.5445\n",
      "Epoch 203/1000\n",
      "80/80 [==============================] - 0s 440us/sample - loss: 5.2619 - val_loss: 8.4658\n",
      "Epoch 204/1000\n",
      "80/80 [==============================] - 0s 345us/sample - loss: 5.3637 - val_loss: 6.4632\n",
      "Epoch 205/1000\n",
      "80/80 [==============================] - 0s 332us/sample - loss: 5.6850 - val_loss: 7.4065\n",
      "Epoch 206/1000\n",
      "80/80 [==============================] - 0s 377us/sample - loss: 5.9855 - val_loss: 7.6767\n",
      "Epoch 207/1000\n",
      "80/80 [==============================] - 0s 383us/sample - loss: 4.9289 - val_loss: 8.3819\n",
      "Epoch 208/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 6.277 - 0s 365us/sample - loss: 5.4190 - val_loss: 6.3442\n",
      "Epoch 209/1000\n",
      "80/80 [==============================] - 0s 432us/sample - loss: 5.4128 - val_loss: 6.5435\n",
      "Epoch 210/1000\n",
      "80/80 [==============================] - 0s 338us/sample - loss: 5.1069 - val_loss: 9.4293\n",
      "Epoch 211/1000\n",
      "80/80 [==============================] - 0s 375us/sample - loss: 5.6282 - val_loss: 8.2996\n",
      "Epoch 212/1000\n",
      "80/80 [==============================] - 0s 417us/sample - loss: 4.9313 - val_loss: 6.9921\n",
      "Epoch 213/1000\n",
      "80/80 [==============================] - 0s 296us/sample - loss: 5.5487 - val_loss: 6.0917\n",
      "Epoch 214/1000\n",
      "80/80 [==============================] - 0s 448us/sample - loss: 4.9495 - val_loss: 8.8402\n",
      "Epoch 215/1000\n",
      "80/80 [==============================] - 0s 373us/sample - loss: 5.4257 - val_loss: 7.8131\n",
      "Epoch 216/1000\n",
      "80/80 [==============================] - 0s 334us/sample - loss: 4.6660 - val_loss: 7.0431\n",
      "Epoch 217/1000\n",
      "80/80 [==============================] - 0s 305us/sample - loss: 4.6151 - val_loss: 6.6892\n",
      "Epoch 218/1000\n",
      "80/80 [==============================] - 0s 576us/sample - loss: 4.5743 - val_loss: 6.9257\n",
      "Epoch 219/1000\n",
      "80/80 [==============================] - 0s 325us/sample - loss: 4.8548 - val_loss: 6.8708\n",
      "Epoch 220/1000\n",
      "80/80 [==============================] - 0s 361us/sample - loss: 4.4752 - val_loss: 7.3207\n",
      "Epoch 221/1000\n",
      "80/80 [==============================] - 0s 285us/sample - loss: 4.4997 - val_loss: 7.2391\n",
      "Epoch 222/1000\n",
      "80/80 [==============================] - 0s 471us/sample - loss: 4.4638 - val_loss: 7.2657\n",
      "Epoch 223/1000\n",
      "80/80 [==============================] - 0s 246us/sample - loss: 4.4499 - val_loss: 7.2530\n",
      "Epoch 224/1000\n",
      "80/80 [==============================] - 0s 394us/sample - loss: 4.4538 - val_loss: 6.5493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/1000\n",
      "80/80 [==============================] - 0s 462us/sample - loss: 4.5956 - val_loss: 5.9199\n",
      "Epoch 226/1000\n",
      "80/80 [==============================] - 0s 469us/sample - loss: 4.4763 - val_loss: 6.5327\n",
      "Epoch 227/1000\n",
      "80/80 [==============================] - 0s 336us/sample - loss: 4.6275 - val_loss: 8.0732\n",
      "Epoch 228/1000\n",
      "80/80 [==============================] - 0s 485us/sample - loss: 4.4973 - val_loss: 6.8854\n",
      "Epoch 229/1000\n",
      "80/80 [==============================] - 0s 542us/sample - loss: 4.2436 - val_loss: 6.0973\n",
      "Epoch 230/1000\n",
      "80/80 [==============================] - 0s 365us/sample - loss: 4.6086 - val_loss: 5.9384\n",
      "Epoch 231/1000\n",
      "80/80 [==============================] - 0s 493us/sample - loss: 4.4417 - val_loss: 8.4519\n",
      "Epoch 232/1000\n",
      "80/80 [==============================] - 0s 570us/sample - loss: 4.6773 - val_loss: 7.2588\n",
      "Epoch 233/1000\n",
      "80/80 [==============================] - 0s 369us/sample - loss: 4.0532 - val_loss: 5.7137\n",
      "Epoch 234/1000\n",
      "80/80 [==============================] - 0s 317us/sample - loss: 5.3190 - val_loss: 5.2815\n",
      "Epoch 235/1000\n",
      "80/80 [==============================] - 0s 481us/sample - loss: 4.6409 - val_loss: 8.5172\n",
      "Epoch 236/1000\n",
      "80/80 [==============================] - 0s 398us/sample - loss: 4.4814 - val_loss: 8.5371\n",
      "Epoch 237/1000\n",
      "80/80 [==============================] - 0s 316us/sample - loss: 4.6257 - val_loss: 6.8634\n",
      "Epoch 238/1000\n",
      "80/80 [==============================] - 0s 461us/sample - loss: 4.4444 - val_loss: 5.6925\n",
      "Epoch 239/1000\n",
      "80/80 [==============================] - 0s 413us/sample - loss: 4.2861 - val_loss: 6.5317\n",
      "Epoch 240/1000\n",
      "80/80 [==============================] - 0s 462us/sample - loss: 4.2448 - val_loss: 6.8150\n",
      "Epoch 241/1000\n",
      "80/80 [==============================] - 0s 447us/sample - loss: 4.3627 - val_loss: 6.3195\n",
      "Epoch 242/1000\n",
      "80/80 [==============================] - 0s 293us/sample - loss: 4.1847 - val_loss: 5.9993\n",
      "Epoch 243/1000\n",
      "80/80 [==============================] - 0s 558us/sample - loss: 4.2865 - val_loss: 5.4417\n",
      "Epoch 244/1000\n",
      "80/80 [==============================] - 0s 573us/sample - loss: 4.2168 - val_loss: 6.4174\n",
      "Epoch 245/1000\n",
      "80/80 [==============================] - 0s 380us/sample - loss: 4.1781 - val_loss: 9.2001\n",
      "Epoch 246/1000\n",
      "80/80 [==============================] - 0s 568us/sample - loss: 4.4187 - val_loss: 6.8188\n",
      "Epoch 247/1000\n",
      "80/80 [==============================] - 0s 373us/sample - loss: 4.7096 - val_loss: 5.3875\n",
      "Epoch 248/1000\n",
      "80/80 [==============================] - 0s 380us/sample - loss: 4.0567 - val_loss: 7.2099\n",
      "Epoch 249/1000\n",
      "80/80 [==============================] - 0s 402us/sample - loss: 4.2255 - val_loss: 7.6294\n",
      "Epoch 250/1000\n",
      "80/80 [==============================] - 0s 510us/sample - loss: 4.1551 - val_loss: 6.4968\n",
      "Epoch 251/1000\n",
      "80/80 [==============================] - 0s 495us/sample - loss: 4.3929 - val_loss: 5.4102\n",
      "Epoch 252/1000\n",
      "80/80 [==============================] - 0s 344us/sample - loss: 3.8734 - val_loss: 6.7447\n",
      "Epoch 253/1000\n",
      "80/80 [==============================] - 0s 426us/sample - loss: 4.2452 - val_loss: 8.8677\n",
      "Epoch 254/1000\n",
      "80/80 [==============================] - 0s 416us/sample - loss: 4.2706 - val_loss: 6.0635\n",
      "Epoch 255/1000\n",
      "80/80 [==============================] - 0s 402us/sample - loss: 4.1987 - val_loss: 5.3919\n",
      "Epoch 256/1000\n",
      "80/80 [==============================] - 0s 470us/sample - loss: 3.7619 - val_loss: 6.9049\n",
      "Epoch 257/1000\n",
      "80/80 [==============================] - 0s 340us/sample - loss: 4.8712 - val_loss: 7.4420\n",
      "Epoch 258/1000\n",
      "80/80 [==============================] - 0s 354us/sample - loss: 3.7036 - val_loss: 6.7401\n",
      "Epoch 259/1000\n",
      "80/80 [==============================] - 0s 370us/sample - loss: 4.4274 - val_loss: 5.7662\n",
      "Epoch 260/1000\n",
      "80/80 [==============================] - 0s 558us/sample - loss: 3.6152 - val_loss: 6.8224\n",
      "Epoch 261/1000\n",
      "80/80 [==============================] - 0s 631us/sample - loss: 4.3284 - val_loss: 5.9189\n",
      "Epoch 262/1000\n",
      "80/80 [==============================] - 0s 454us/sample - loss: 3.9977 - val_loss: 5.0576\n",
      "Epoch 263/1000\n",
      "80/80 [==============================] - 0s 359us/sample - loss: 4.0472 - val_loss: 6.0972\n",
      "Epoch 264/1000\n",
      "80/80 [==============================] - 0s 320us/sample - loss: 3.7727 - val_loss: 8.2510\n",
      "Epoch 265/1000\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 4.2411 - val_loss: 5.9527\n",
      "Epoch 266/1000\n",
      "80/80 [==============================] - 0s 440us/sample - loss: 3.8283 - val_loss: 4.8058\n",
      "Epoch 267/1000\n",
      "80/80 [==============================] - 0s 401us/sample - loss: 4.0122 - val_loss: 5.3572\n",
      "Epoch 268/1000\n",
      "80/80 [==============================] - 0s 384us/sample - loss: 3.6965 - val_loss: 6.8348\n",
      "Epoch 269/1000\n",
      "80/80 [==============================] - 0s 511us/sample - loss: 3.8704 - val_loss: 6.2707\n",
      "Epoch 270/1000\n",
      "80/80 [==============================] - 0s 348us/sample - loss: 3.6210 - val_loss: 6.1681\n",
      "Epoch 271/1000\n",
      "80/80 [==============================] - 0s 310us/sample - loss: 3.6144 - val_loss: 6.2158\n",
      "Epoch 272/1000\n",
      "80/80 [==============================] - 0s 355us/sample - loss: 3.7612 - val_loss: 6.2150\n",
      "Epoch 273/1000\n",
      "80/80 [==============================] - 0s 309us/sample - loss: 3.5910 - val_loss: 5.9352\n",
      "Epoch 274/1000\n",
      "80/80 [==============================] - 0s 341us/sample - loss: 3.6433 - val_loss: 5.8493\n",
      "Epoch 275/1000\n",
      "80/80 [==============================] - 0s 645us/sample - loss: 3.5176 - val_loss: 6.7424\n",
      "Epoch 276/1000\n",
      "80/80 [==============================] - 0s 321us/sample - loss: 3.7763 - val_loss: 6.2485\n",
      "Epoch 277/1000\n",
      "80/80 [==============================] - 0s 356us/sample - loss: 3.9748 - val_loss: 4.7759\n",
      "Epoch 278/1000\n",
      "80/80 [==============================] - 0s 404us/sample - loss: 3.8523 - val_loss: 5.8719\n",
      "Epoch 279/1000\n",
      "80/80 [==============================] - 0s 316us/sample - loss: 3.6686 - val_loss: 6.7912\n",
      "Epoch 280/1000\n",
      "80/80 [==============================] - 0s 431us/sample - loss: 3.9152 - val_loss: 5.3178\n",
      "Epoch 281/1000\n",
      "80/80 [==============================] - 0s 327us/sample - loss: 3.7407 - val_loss: 6.2698\n",
      "Epoch 282/1000\n",
      "80/80 [==============================] - 0s 380us/sample - loss: 3.8205 - val_loss: 6.0297\n",
      "Epoch 283/1000\n",
      "80/80 [==============================] - 0s 375us/sample - loss: 3.6329 - val_loss: 6.0101\n",
      "Epoch 284/1000\n",
      "80/80 [==============================] - 0s 431us/sample - loss: 3.8668 - val_loss: 6.2669\n",
      "Epoch 285/1000\n",
      "80/80 [==============================] - 0s 329us/sample - loss: 3.7100 - val_loss: 5.8159\n",
      "Epoch 286/1000\n",
      "80/80 [==============================] - 0s 379us/sample - loss: 3.7104 - val_loss: 5.6034\n",
      "Epoch 287/1000\n",
      "80/80 [==============================] - 0s 356us/sample - loss: 3.8520 - val_loss: 5.7700\n",
      "Epoch 288/1000\n",
      "80/80 [==============================] - 0s 375us/sample - loss: 3.7373 - val_loss: 4.7162\n",
      "Epoch 289/1000\n",
      "80/80 [==============================] - 0s 392us/sample - loss: 3.9056 - val_loss: 5.9432\n",
      "Epoch 290/1000\n",
      "80/80 [==============================] - 0s 347us/sample - loss: 4.3929 - val_loss: 7.6710\n",
      "Epoch 291/1000\n",
      "80/80 [==============================] - 0s 622us/sample - loss: 3.7471 - val_loss: 4.8132\n",
      "Epoch 292/1000\n",
      "80/80 [==============================] - 0s 511us/sample - loss: 3.6555 - val_loss: 5.1683\n",
      "Epoch 293/1000\n",
      "80/80 [==============================] - 0s 349us/sample - loss: 3.6605 - val_loss: 7.5811\n",
      "Epoch 294/1000\n",
      "80/80 [==============================] - 0s 459us/sample - loss: 3.6712 - val_loss: 6.1742\n",
      "Epoch 295/1000\n",
      "80/80 [==============================] - 0s 346us/sample - loss: 3.4520 - val_loss: 4.7385\n",
      "Epoch 296/1000\n",
      "80/80 [==============================] - 0s 411us/sample - loss: 3.5067 - val_loss: 5.3549\n",
      "Epoch 297/1000\n",
      "80/80 [==============================] - 0s 280us/sample - loss: 3.3583 - val_loss: 7.0356\n",
      "Epoch 298/1000\n",
      "80/80 [==============================] - 0s 387us/sample - loss: 3.5398 - val_loss: 5.9106\n",
      "Epoch 299/1000\n",
      "80/80 [==============================] - 0s 362us/sample - loss: 3.3969 - val_loss: 4.7816\n",
      "Epoch 300/1000\n",
      "80/80 [==============================] - 0s 322us/sample - loss: 3.8800 - val_loss: 5.3311\n",
      "Epoch 301/1000\n",
      "80/80 [==============================] - 0s 280us/sample - loss: 3.2742 - val_loss: 7.7970\n",
      "Epoch 302/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 274us/sample - loss: 3.6586 - val_loss: 6.5442\n",
      "Epoch 303/1000\n",
      "80/80 [==============================] - 0s 276us/sample - loss: 3.4006 - val_loss: 4.6712\n",
      "Epoch 304/1000\n",
      "80/80 [==============================] - 0s 393us/sample - loss: 3.5565 - val_loss: 4.5481\n",
      "Epoch 305/1000\n",
      "80/80 [==============================] - 0s 448us/sample - loss: 3.5188 - val_loss: 6.5207\n",
      "Epoch 306/1000\n",
      "80/80 [==============================] - 0s 479us/sample - loss: 3.7101 - val_loss: 6.5342\n",
      "Epoch 307/1000\n",
      "80/80 [==============================] - 0s 559us/sample - loss: 3.4674 - val_loss: 5.7584\n",
      "Epoch 308/1000\n",
      "80/80 [==============================] - 0s 386us/sample - loss: 3.4329 - val_loss: 5.4419\n",
      "Epoch 309/1000\n",
      "80/80 [==============================] - 0s 475us/sample - loss: 3.3603 - val_loss: 5.1077\n",
      "Epoch 310/1000\n",
      "80/80 [==============================] - 0s 402us/sample - loss: 3.5689 - val_loss: 5.1552\n",
      "Epoch 311/1000\n",
      "80/80 [==============================] - 0s 414us/sample - loss: 3.2314 - val_loss: 5.4126\n",
      "Epoch 312/1000\n",
      "80/80 [==============================] - 0s 481us/sample - loss: 3.5889 - val_loss: 5.4002\n",
      "Epoch 313/1000\n",
      "80/80 [==============================] - 0s 369us/sample - loss: 3.3681 - val_loss: 5.2125\n",
      "Epoch 314/1000\n",
      "80/80 [==============================] - 0s 309us/sample - loss: 3.3425 - val_loss: 5.6934\n",
      "Epoch 315/1000\n",
      "80/80 [==============================] - 0s 389us/sample - loss: 3.4107 - val_loss: 4.9700\n",
      "Epoch 316/1000\n",
      "80/80 [==============================] - 0s 366us/sample - loss: 3.2211 - val_loss: 4.9039\n",
      "Epoch 317/1000\n",
      "80/80 [==============================] - 0s 448us/sample - loss: 3.2782 - val_loss: 5.6590\n",
      "Epoch 318/1000\n",
      "80/80 [==============================] - 0s 370us/sample - loss: 3.3243 - val_loss: 5.1962\n",
      "Epoch 319/1000\n",
      "80/80 [==============================] - 0s 341us/sample - loss: 3.2981 - val_loss: 6.2220\n",
      "Epoch 320/1000\n",
      "80/80 [==============================] - 0s 596us/sample - loss: 3.2790 - val_loss: 4.8310\n",
      "Epoch 321/1000\n",
      "80/80 [==============================] - 0s 585us/sample - loss: 3.1906 - val_loss: 4.6037\n",
      "Epoch 322/1000\n",
      "80/80 [==============================] - 0s 401us/sample - loss: 3.4853 - val_loss: 5.6504\n",
      "Epoch 323/1000\n",
      "80/80 [==============================] - 0s 376us/sample - loss: 3.1750 - val_loss: 5.4489\n",
      "Epoch 324/1000\n",
      "80/80 [==============================] - 0s 483us/sample - loss: 3.3363 - val_loss: 5.5170\n",
      "Epoch 325/1000\n",
      "80/80 [==============================] - 0s 412us/sample - loss: 3.2895 - val_loss: 5.8807\n",
      "Epoch 326/1000\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 3.2794 - val_loss: 5.1735\n",
      "Epoch 327/1000\n",
      "80/80 [==============================] - 0s 504us/sample - loss: 3.1763 - val_loss: 4.8555\n",
      "Epoch 328/1000\n",
      "80/80 [==============================] - 0s 433us/sample - loss: 3.3614 - val_loss: 4.8843\n",
      "Epoch 329/1000\n",
      "80/80 [==============================] - 0s 390us/sample - loss: 3.1554 - val_loss: 5.9739\n",
      "Epoch 330/1000\n",
      "80/80 [==============================] - 0s 564us/sample - loss: 3.2642 - val_loss: 6.0463\n",
      "Epoch 331/1000\n",
      "80/80 [==============================] - 0s 486us/sample - loss: 3.2058 - val_loss: 4.7084\n",
      "Epoch 332/1000\n",
      "80/80 [==============================] - 0s 303us/sample - loss: 3.3143 - val_loss: 5.0595\n",
      "Epoch 333/1000\n",
      "80/80 [==============================] - 0s 414us/sample - loss: 3.3062 - val_loss: 4.8057\n",
      "Epoch 334/1000\n",
      "80/80 [==============================] - 0s 440us/sample - loss: 3.4797 - val_loss: 5.3602\n",
      "Epoch 335/1000\n",
      "80/80 [==============================] - 0s 538us/sample - loss: 3.3836 - val_loss: 6.0661\n",
      "Epoch 336/1000\n",
      "80/80 [==============================] - 0s 384us/sample - loss: 3.1750 - val_loss: 5.5635\n",
      "Epoch 337/1000\n",
      "80/80 [==============================] - 0s 421us/sample - loss: 3.1209 - val_loss: 5.0474\n",
      "Epoch 338/1000\n",
      "80/80 [==============================] - 0s 369us/sample - loss: 3.1311 - val_loss: 5.1979\n",
      "Epoch 339/1000\n",
      "80/80 [==============================] - 0s 395us/sample - loss: 3.4333 - val_loss: 5.6696\n",
      "Epoch 340/1000\n",
      "80/80 [==============================] - 0s 303us/sample - loss: 3.5587 - val_loss: 4.0033\n",
      "Epoch 341/1000\n",
      "80/80 [==============================] - 0s 375us/sample - loss: 3.6613 - val_loss: 5.0402\n",
      "Epoch 342/1000\n",
      "80/80 [==============================] - 0s 495us/sample - loss: 3.5996 - val_loss: 9.1983\n",
      "Epoch 343/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.490 - 0s 336us/sample - loss: 3.9056 - val_loss: 5.5382\n",
      "Epoch 344/1000\n",
      "80/80 [==============================] - 0s 299us/sample - loss: 3.5856 - val_loss: 3.6978\n",
      "Epoch 345/1000\n",
      "80/80 [==============================] - 0s 312us/sample - loss: 3.3914 - val_loss: 6.5191\n",
      "Epoch 346/1000\n",
      "80/80 [==============================] - 0s 340us/sample - loss: 3.7169 - val_loss: 6.5066\n",
      "Epoch 347/1000\n",
      "80/80 [==============================] - 0s 360us/sample - loss: 3.6304 - val_loss: 3.9336\n",
      "Epoch 348/1000\n",
      "80/80 [==============================] - 0s 414us/sample - loss: 3.4375 - val_loss: 5.0893\n",
      "Epoch 349/1000\n",
      "80/80 [==============================] - 0s 586us/sample - loss: 3.2613 - val_loss: 5.4940\n",
      "Epoch 350/1000\n",
      "80/80 [==============================] - 0s 500us/sample - loss: 3.1942 - val_loss: 6.1504\n",
      "Epoch 351/1000\n",
      "80/80 [==============================] - 0s 354us/sample - loss: 3.3641 - val_loss: 4.8742\n",
      "Epoch 352/1000\n",
      "80/80 [==============================] - 0s 405us/sample - loss: 3.4091 - val_loss: 4.2181\n",
      "Epoch 353/1000\n",
      "80/80 [==============================] - 0s 390us/sample - loss: 3.1935 - val_loss: 4.9102\n",
      "Epoch 354/1000\n",
      "80/80 [==============================] - 0s 373us/sample - loss: 3.1869 - val_loss: 4.6878\n",
      "Epoch 355/1000\n",
      "80/80 [==============================] - 0s 264us/sample - loss: 3.1076 - val_loss: 4.8852\n",
      "Epoch 356/1000\n",
      "80/80 [==============================] - 0s 263us/sample - loss: 3.2259 - val_loss: 4.2631\n",
      "Epoch 357/1000\n",
      "80/80 [==============================] - 0s 414us/sample - loss: 3.1361 - val_loss: 5.2048\n",
      "Epoch 358/1000\n",
      "80/80 [==============================] - 0s 340us/sample - loss: 3.4958 - val_loss: 5.8951\n",
      "Epoch 359/1000\n",
      "80/80 [==============================] - 0s 301us/sample - loss: 3.6416 - val_loss: 4.3634\n",
      "Epoch 360/1000\n",
      "80/80 [==============================] - 0s 488us/sample - loss: 3.3285 - val_loss: 4.7313\n",
      "Epoch 361/1000\n",
      "80/80 [==============================] - 0s 362us/sample - loss: 4.0911 - val_loss: 5.3581\n",
      "Epoch 362/1000\n",
      "80/80 [==============================] - 0s 386us/sample - loss: 2.9123 - val_loss: 5.7123\n",
      "Epoch 363/1000\n",
      "80/80 [==============================] - 0s 392us/sample - loss: 3.9064 - val_loss: 5.6470\n",
      "Epoch 364/1000\n",
      "80/80 [==============================] - 0s 338us/sample - loss: 3.0995 - val_loss: 6.8673\n",
      "Epoch 365/1000\n",
      "80/80 [==============================] - 0s 619us/sample - loss: 4.5615 - val_loss: 3.6203\n",
      "Epoch 366/1000\n",
      "80/80 [==============================] - 0s 415us/sample - loss: 3.5366 - val_loss: 5.7651\n",
      "Epoch 367/1000\n",
      "80/80 [==============================] - 0s 695us/sample - loss: 4.1274 - val_loss: 6.6296\n",
      "Epoch 368/1000\n",
      "80/80 [==============================] - 0s 308us/sample - loss: 4.2590 - val_loss: 5.7025\n",
      "Epoch 369/1000\n",
      "80/80 [==============================] - 0s 367us/sample - loss: 2.9289 - val_loss: 4.6947\n",
      "Epoch 370/1000\n",
      "80/80 [==============================] - 0s 408us/sample - loss: 4.2924 - val_loss: 4.9004\n",
      "Epoch 371/1000\n",
      "80/80 [==============================] - 0s 341us/sample - loss: 3.6390 - val_loss: 7.7369\n",
      "Epoch 372/1000\n",
      "80/80 [==============================] - 0s 435us/sample - loss: 3.7080 - val_loss: 4.3618\n",
      "Epoch 373/1000\n",
      "80/80 [==============================] - 0s 298us/sample - loss: 3.6519 - val_loss: 4.1185\n",
      "Epoch 374/1000\n",
      "80/80 [==============================] - 0s 265us/sample - loss: 3.4968 - val_loss: 5.8805\n",
      "Epoch 375/1000\n",
      "80/80 [==============================] - 0s 371us/sample - loss: 4.2219 - val_loss: 5.3682\n",
      "Epoch 376/1000\n",
      "80/80 [==============================] - 0s 353us/sample - loss: 3.0915 - val_loss: 3.8829\n",
      "Epoch 377/1000\n",
      "80/80 [==============================] - 0s 468us/sample - loss: 3.6080 - val_loss: 4.3545\n",
      "Epoch 378/1000\n",
      "80/80 [==============================] - 0s 424us/sample - loss: 3.0625 - val_loss: 4.9660\n",
      "Epoch 379/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 364us/sample - loss: 2.9122 - val_loss: 4.2528\n",
      "Epoch 380/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.354 - 0s 377us/sample - loss: 3.0160 - val_loss: 4.8145\n",
      "Epoch 381/1000\n",
      "80/80 [==============================] - 0s 560us/sample - loss: 2.9783 - val_loss: 4.7525\n",
      "Epoch 382/1000\n",
      "80/80 [==============================] - 0s 494us/sample - loss: 3.3253 - val_loss: 4.1351\n",
      "Epoch 383/1000\n",
      "80/80 [==============================] - 0s 401us/sample - loss: 2.9777 - val_loss: 6.1268\n",
      "Epoch 384/1000\n",
      "80/80 [==============================] - 0s 536us/sample - loss: 3.6897 - val_loss: 6.0404\n",
      "Epoch 385/1000\n",
      "80/80 [==============================] - 0s 357us/sample - loss: 3.0826 - val_loss: 4.8206\n",
      "Epoch 386/1000\n",
      "80/80 [==============================] - 0s 431us/sample - loss: 3.4627 - val_loss: 3.8365\n",
      "Epoch 387/1000\n",
      "80/80 [==============================] - 0s 286us/sample - loss: 3.3344 - val_loss: 5.4121\n",
      "Epoch 388/1000\n",
      "80/80 [==============================] - 0s 413us/sample - loss: 3.0837 - val_loss: 5.0530\n",
      "Epoch 389/1000\n",
      "80/80 [==============================] - 0s 406us/sample - loss: 4.2389 - val_loss: 4.3705\n",
      "Epoch 390/1000\n",
      "80/80 [==============================] - 0s 333us/sample - loss: 2.9305 - val_loss: 6.1638\n",
      "Epoch 391/1000\n",
      "80/80 [==============================] - 0s 491us/sample - loss: 4.1756 - val_loss: 5.8047\n",
      "Epoch 392/1000\n",
      "80/80 [==============================] - 0s 363us/sample - loss: 3.4792 - val_loss: 4.1723\n",
      "Epoch 393/1000\n",
      "80/80 [==============================] - 0s 339us/sample - loss: 3.2557 - val_loss: 4.5202\n",
      "Epoch 394/1000\n",
      "80/80 [==============================] - 0s 410us/sample - loss: 3.3457 - val_loss: 5.1833\n",
      "Epoch 395/1000\n",
      "80/80 [==============================] - 0s 343us/sample - loss: 2.8097 - val_loss: 6.2600\n",
      "Epoch 396/1000\n",
      "80/80 [==============================] - 0s 654us/sample - loss: 3.8192 - val_loss: 3.3937\n",
      "Epoch 397/1000\n",
      "80/80 [==============================] - 0s 374us/sample - loss: 3.1646 - val_loss: 3.8349\n",
      "Epoch 398/1000\n",
      "80/80 [==============================] - 0s 472us/sample - loss: 3.1701 - val_loss: 5.7699\n",
      "Epoch 399/1000\n",
      "80/80 [==============================] - 0s 461us/sample - loss: 3.6826 - val_loss: 5.2048\n",
      "Epoch 400/1000\n",
      "80/80 [==============================] - 0s 263us/sample - loss: 2.8542 - val_loss: 3.6950\n",
      "Epoch 401/1000\n",
      "80/80 [==============================] - 0s 475us/sample - loss: 3.9576 - val_loss: 4.8629\n",
      "Epoch 402/1000\n",
      "80/80 [==============================] - 0s 360us/sample - loss: 2.8687 - val_loss: 6.2086\n",
      "Epoch 403/1000\n",
      "80/80 [==============================] - 0s 464us/sample - loss: 3.2267 - val_loss: 3.6021\n",
      "Epoch 404/1000\n",
      "80/80 [==============================] - 0s 368us/sample - loss: 3.2181 - val_loss: 3.9413\n",
      "Epoch 405/1000\n",
      "80/80 [==============================] - 0s 506us/sample - loss: 3.0041 - val_loss: 5.5193\n",
      "Epoch 406/1000\n",
      "80/80 [==============================] - 0s 415us/sample - loss: 2.8654 - val_loss: 5.0929\n",
      "Epoch 407/1000\n",
      "80/80 [==============================] - 0s 260us/sample - loss: 2.8828 - val_loss: 3.5609\n",
      "Epoch 408/1000\n",
      "80/80 [==============================] - 0s 412us/sample - loss: 3.1118 - val_loss: 4.2974\n",
      "Epoch 409/1000\n",
      "80/80 [==============================] - 0s 412us/sample - loss: 3.0261 - val_loss: 4.3020\n",
      "Epoch 410/1000\n",
      "80/80 [==============================] - 0s 513us/sample - loss: 2.8239 - val_loss: 4.3640\n",
      "Epoch 411/1000\n",
      "80/80 [==============================] - 0s 526us/sample - loss: 2.8890 - val_loss: 5.6255\n",
      "Epoch 412/1000\n",
      "80/80 [==============================] - 0s 383us/sample - loss: 2.9148 - val_loss: 4.9486\n",
      "Epoch 413/1000\n",
      "80/80 [==============================] - 0s 514us/sample - loss: 2.8324 - val_loss: 3.6951\n",
      "Epoch 414/1000\n",
      "80/80 [==============================] - 0s 383us/sample - loss: 3.0112 - val_loss: 4.4259\n",
      "Epoch 415/1000\n",
      "80/80 [==============================] - 0s 386us/sample - loss: 2.9580 - val_loss: 3.7379\n",
      "Epoch 416/1000\n",
      "80/80 [==============================] - 0s 341us/sample - loss: 2.7364 - val_loss: 5.6122\n",
      "Epoch 417/1000\n",
      "80/80 [==============================] - 0s 304us/sample - loss: 3.0403 - val_loss: 6.7805\n",
      "Epoch 418/1000\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 2.9079 - val_loss: 3.8757\n",
      "Epoch 419/1000\n",
      "80/80 [==============================] - 0s 377us/sample - loss: 3.1990 - val_loss: 3.1964\n",
      "Epoch 420/1000\n",
      "80/80 [==============================] - 0s 307us/sample - loss: 3.3021 - val_loss: 5.3496\n",
      "Epoch 421/1000\n",
      "80/80 [==============================] - 0s 386us/sample - loss: 3.0402 - val_loss: 8.9374\n",
      "Epoch 422/1000\n",
      "80/80 [==============================] - 0s 417us/sample - loss: 3.6507 - val_loss: 3.1761\n",
      "Epoch 423/1000\n",
      "80/80 [==============================] - 0s 357us/sample - loss: 3.1159 - val_loss: 3.6963\n",
      "Epoch 424/1000\n",
      "80/80 [==============================] - 0s 419us/sample - loss: 3.4064 - val_loss: 6.0820\n",
      "Epoch 425/1000\n",
      "80/80 [==============================] - 0s 537us/sample - loss: 3.1195 - val_loss: 5.1375\n",
      "Epoch 426/1000\n",
      "80/80 [==============================] - 0s 478us/sample - loss: 3.3129 - val_loss: 3.0720\n",
      "Epoch 427/1000\n",
      "80/80 [==============================] - 0s 435us/sample - loss: 3.0623 - val_loss: 4.7168\n",
      "Epoch 428/1000\n",
      "80/80 [==============================] - 0s 497us/sample - loss: 2.7476 - val_loss: 5.2589\n",
      "Epoch 429/1000\n",
      "80/80 [==============================] - 0s 358us/sample - loss: 3.2084 - val_loss: 3.6758\n",
      "Epoch 430/1000\n",
      "80/80 [==============================] - 0s 373us/sample - loss: 2.6622 - val_loss: 5.4346\n",
      "Epoch 431/1000\n",
      "80/80 [==============================] - 0s 256us/sample - loss: 2.9391 - val_loss: 6.8766\n",
      "Epoch 432/1000\n",
      "80/80 [==============================] - 0s 271us/sample - loss: 3.0789 - val_loss: 4.5994\n",
      "Epoch 433/1000\n",
      "80/80 [==============================] - 0s 325us/sample - loss: 2.6750 - val_loss: 3.6046\n",
      "Epoch 434/1000\n",
      "80/80 [==============================] - 0s 408us/sample - loss: 2.8791 - val_loss: 4.2052\n",
      "Epoch 435/1000\n",
      "80/80 [==============================] - 0s 321us/sample - loss: 2.6114 - val_loss: 5.0160\n",
      "Epoch 436/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 1.095 - 0s 273us/sample - loss: 2.8587 - val_loss: 3.9554\n",
      "Epoch 437/1000\n",
      "80/80 [==============================] - 0s 347us/sample - loss: 3.1914 - val_loss: 3.5499\n",
      "Epoch 438/1000\n",
      "80/80 [==============================] - 0s 376us/sample - loss: 2.7876 - val_loss: 4.8125\n",
      "Epoch 439/1000\n",
      "80/80 [==============================] - 0s 434us/sample - loss: 3.3124 - val_loss: 4.6220\n",
      "Epoch 440/1000\n",
      "80/80 [==============================] - 0s 360us/sample - loss: 2.8371 - val_loss: 4.6088\n",
      "Epoch 441/1000\n",
      "80/80 [==============================] - 0s 633us/sample - loss: 2.6875 - val_loss: 4.2982\n",
      "Epoch 442/1000\n",
      "80/80 [==============================] - 0s 545us/sample - loss: 2.8005 - val_loss: 4.0616\n",
      "Epoch 443/1000\n",
      "80/80 [==============================] - 0s 279us/sample - loss: 2.7552 - val_loss: 4.0342\n",
      "Epoch 444/1000\n",
      "80/80 [==============================] - 0s 491us/sample - loss: 2.8969 - val_loss: 4.4199\n",
      "Epoch 445/1000\n",
      "80/80 [==============================] - 0s 340us/sample - loss: 2.8160 - val_loss: 5.2684\n",
      "Epoch 446/1000\n",
      "80/80 [==============================] - 0s 366us/sample - loss: 2.8508 - val_loss: 4.3433\n",
      "Epoch 447/1000\n",
      "80/80 [==============================] - 0s 345us/sample - loss: 2.7913 - val_loss: 4.0116\n",
      "Epoch 448/1000\n",
      "80/80 [==============================] - 0s 404us/sample - loss: 2.7691 - val_loss: 5.0841\n",
      "Epoch 449/1000\n",
      "80/80 [==============================] - 0s 513us/sample - loss: 2.7094 - val_loss: 5.4504\n",
      "Epoch 450/1000\n",
      "80/80 [==============================] - 0s 285us/sample - loss: 2.7186 - val_loss: 4.5441\n",
      "Epoch 451/1000\n",
      "80/80 [==============================] - 0s 340us/sample - loss: 2.6135 - val_loss: 3.0782\n",
      "Epoch 452/1000\n",
      "80/80 [==============================] - 0s 291us/sample - loss: 3.1783 - val_loss: 3.9873\n",
      "Epoch 453/1000\n",
      "80/80 [==============================] - 0s 401us/sample - loss: 2.8669 - val_loss: 5.9488\n",
      "Epoch 454/1000\n",
      "80/80 [==============================] - 0s 306us/sample - loss: 2.6363 - val_loss: 3.7854\n",
      "Epoch 455/1000\n",
      "80/80 [==============================] - 0s 430us/sample - loss: 3.0235 - val_loss: 3.0996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 456/1000\n",
      "80/80 [==============================] - 0s 348us/sample - loss: 3.1838 - val_loss: 5.0922\n",
      "Epoch 457/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 1.735 - 0s 558us/sample - loss: 3.1130 - val_loss: 6.9364\n",
      "Epoch 458/1000\n",
      "80/80 [==============================] - 0s 378us/sample - loss: 2.6932 - val_loss: 2.6211\n",
      "Epoch 459/1000\n",
      "80/80 [==============================] - 0s 360us/sample - loss: 3.7397 - val_loss: 2.9457\n",
      "Epoch 460/1000\n",
      "80/80 [==============================] - 0s 475us/sample - loss: 2.7223 - val_loss: 6.8830\n",
      "Epoch 461/1000\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 3.4945 - val_loss: 5.2285\n",
      "Epoch 462/1000\n",
      "80/80 [==============================] - 0s 395us/sample - loss: 2.7819 - val_loss: 2.7178\n",
      "Epoch 463/1000\n",
      "80/80 [==============================] - 0s 465us/sample - loss: 3.0716 - val_loss: 4.0246\n",
      "Epoch 464/1000\n",
      "80/80 [==============================] - 0s 327us/sample - loss: 2.6069 - val_loss: 6.0018\n",
      "Epoch 465/1000\n",
      "80/80 [==============================] - 0s 434us/sample - loss: 2.7888 - val_loss: 4.2218\n",
      "Epoch 466/1000\n",
      "80/80 [==============================] - 0s 426us/sample - loss: 2.6465 - val_loss: 3.7237\n",
      "Epoch 467/1000\n",
      "80/80 [==============================] - 0s 468us/sample - loss: 2.6103 - val_loss: 4.4305\n",
      "Epoch 468/1000\n",
      "80/80 [==============================] - 0s 395us/sample - loss: 2.6674 - val_loss: 4.7878\n",
      "Epoch 469/1000\n",
      "80/80 [==============================] - 0s 493us/sample - loss: 2.7084 - val_loss: 4.0802\n",
      "Epoch 470/1000\n",
      "80/80 [==============================] - 0s 527us/sample - loss: 2.6215 - val_loss: 3.7443\n",
      "Epoch 471/1000\n",
      "80/80 [==============================] - 0s 575us/sample - loss: 2.5326 - val_loss: 4.9262\n",
      "Epoch 472/1000\n",
      "80/80 [==============================] - 0s 369us/sample - loss: 3.1049 - val_loss: 5.5159\n",
      "Epoch 473/1000\n",
      "80/80 [==============================] - 0s 397us/sample - loss: 3.2218 - val_loss: 4.5012\n",
      "Epoch 474/1000\n",
      "80/80 [==============================] - 0s 394us/sample - loss: 2.9793 - val_loss: 3.7659\n",
      "Epoch 475/1000\n",
      "80/80 [==============================] - 0s 421us/sample - loss: 3.3995 - val_loss: 6.0621\n",
      "Epoch 476/1000\n",
      "80/80 [==============================] - 0s 461us/sample - loss: 2.4536 - val_loss: 5.4818\n",
      "Epoch 477/1000\n",
      "80/80 [==============================] - 0s 314us/sample - loss: 3.2681 - val_loss: 3.3490\n",
      "Epoch 478/1000\n",
      "80/80 [==============================] - 0s 327us/sample - loss: 2.6564 - val_loss: 4.2437\n",
      "Epoch 479/1000\n",
      "80/80 [==============================] - 0s 372us/sample - loss: 2.9814 - val_loss: 5.4015\n",
      "Epoch 480/1000\n",
      "80/80 [==============================] - 0s 373us/sample - loss: 2.6319 - val_loss: 3.1757\n",
      "Epoch 481/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.271 - 0s 374us/sample - loss: 3.4943 - val_loss: 2.7880\n",
      "Epoch 482/1000\n",
      "80/80 [==============================] - 0s 348us/sample - loss: 3.0213 - val_loss: 7.2502\n",
      "Epoch 483/1000\n",
      "80/80 [==============================] - 0s 304us/sample - loss: 3.1513 - val_loss: 4.3927\n",
      "Epoch 484/1000\n",
      "80/80 [==============================] - 0s 557us/sample - loss: 2.5710 - val_loss: 2.7645\n",
      "Epoch 485/1000\n",
      "80/80 [==============================] - 0s 402us/sample - loss: 2.9938 - val_loss: 4.2007\n",
      "Epoch 486/1000\n",
      "80/80 [==============================] - 0s 371us/sample - loss: 2.9958 - val_loss: 6.1493\n",
      "Epoch 487/1000\n",
      "80/80 [==============================] - 0s 336us/sample - loss: 2.5296 - val_loss: 2.9000\n",
      "Epoch 488/1000\n",
      "80/80 [==============================] - 0s 341us/sample - loss: 2.8927 - val_loss: 3.1050\n",
      "Epoch 489/1000\n",
      "80/80 [==============================] - 0s 390us/sample - loss: 2.9831 - val_loss: 4.5161\n",
      "Epoch 490/1000\n",
      "80/80 [==============================] - 0s 400us/sample - loss: 2.8071 - val_loss: 5.0456\n",
      "Epoch 491/1000\n",
      "80/80 [==============================] - 0s 361us/sample - loss: 3.6057 - val_loss: 5.3568\n",
      "Epoch 492/1000\n",
      "80/80 [==============================] - 0s 369us/sample - loss: 2.9043 - val_loss: 4.6175\n",
      "Epoch 493/1000\n",
      "80/80 [==============================] - 0s 374us/sample - loss: 3.0593 - val_loss: 3.0914\n",
      "Epoch 494/1000\n",
      "80/80 [==============================] - 0s 349us/sample - loss: 2.7549 - val_loss: 5.3808\n",
      "Epoch 495/1000\n",
      "80/80 [==============================] - 0s 412us/sample - loss: 2.6463 - val_loss: 5.9108\n",
      "Epoch 496/1000\n",
      "80/80 [==============================] - 0s 500us/sample - loss: 2.7434 - val_loss: 3.3353\n",
      "Epoch 497/1000\n",
      "80/80 [==============================] - 0s 420us/sample - loss: 2.8878 - val_loss: 3.5245\n",
      "Epoch 498/1000\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 2.8766 - val_loss: 4.9976\n",
      "Epoch 499/1000\n",
      "80/80 [==============================] - 0s 551us/sample - loss: 2.5021 - val_loss: 3.0343\n",
      "Epoch 500/1000\n",
      "80/80 [==============================] - 0s 602us/sample - loss: 2.6406 - val_loss: 2.6345\n",
      "Epoch 501/1000\n",
      "80/80 [==============================] - 0s 503us/sample - loss: 2.5868 - val_loss: 5.0683\n",
      "Epoch 502/1000\n",
      "80/80 [==============================] - 0s 479us/sample - loss: 2.9653 - val_loss: 6.7284\n",
      "Epoch 503/1000\n",
      "80/80 [==============================] - 0s 322us/sample - loss: 3.0379 - val_loss: 2.6408\n",
      "Epoch 504/1000\n",
      "80/80 [==============================] - 0s 394us/sample - loss: 3.0296 - val_loss: 3.2153\n",
      "Epoch 505/1000\n",
      "80/80 [==============================] - 0s 347us/sample - loss: 2.8438 - val_loss: 7.0882\n",
      "Epoch 506/1000\n",
      "80/80 [==============================] - 0s 330us/sample - loss: 2.8928 - val_loss: 3.8376\n",
      "Epoch 507/1000\n",
      "80/80 [==============================] - 0s 333us/sample - loss: 3.1223 - val_loss: 2.8718\n",
      "Epoch 508/1000\n",
      "80/80 [==============================] - 0s 445us/sample - loss: 2.5610 - val_loss: 8.3285\n",
      "Epoch 509/1000\n",
      "80/80 [==============================] - 0s 374us/sample - loss: 3.4056 - val_loss: 5.0521\n",
      "Epoch 510/1000\n",
      "80/80 [==============================] - 0s 383us/sample - loss: 2.3377 - val_loss: 2.7300\n",
      "Epoch 511/1000\n",
      "80/80 [==============================] - 0s 337us/sample - loss: 2.7380 - val_loss: 3.8136\n",
      "Epoch 512/1000\n",
      "80/80 [==============================] - 0s 388us/sample - loss: 2.7764 - val_loss: 5.5267\n",
      "Epoch 513/1000\n",
      "80/80 [==============================] - 0s 346us/sample - loss: 2.5565 - val_loss: 4.6089\n",
      "Epoch 514/1000\n",
      "80/80 [==============================] - 0s 264us/sample - loss: 2.5067 - val_loss: 3.8954\n",
      "Epoch 515/1000\n",
      "80/80 [==============================] - 0s 578us/sample - loss: 2.4192 - val_loss: 3.4202\n",
      "Epoch 516/1000\n",
      "80/80 [==============================] - 0s 473us/sample - loss: 2.5421 - val_loss: 3.4195\n",
      "Epoch 517/1000\n",
      "80/80 [==============================] - 0s 318us/sample - loss: 2.3935 - val_loss: 5.5488\n",
      "Epoch 518/1000\n",
      "80/80 [==============================] - 0s 325us/sample - loss: 2.5374 - val_loss: 4.7451\n",
      "Epoch 519/1000\n",
      "80/80 [==============================] - 0s 375us/sample - loss: 2.4388 - val_loss: 3.5067\n",
      "Epoch 520/1000\n",
      "80/80 [==============================] - 0s 413us/sample - loss: 2.6352 - val_loss: 3.7412\n",
      "Epoch 521/1000\n",
      "80/80 [==============================] - 0s 328us/sample - loss: 2.4007 - val_loss: 5.7551\n",
      "Epoch 522/1000\n",
      "80/80 [==============================] - 0s 298us/sample - loss: 2.6754 - val_loss: 4.1079\n",
      "Epoch 523/1000\n",
      "80/80 [==============================] - 0s 364us/sample - loss: 2.8777 - val_loss: 2.8199\n",
      "Epoch 524/1000\n",
      "80/80 [==============================] - 0s 378us/sample - loss: 2.5646 - val_loss: 4.6949\n",
      "Epoch 525/1000\n",
      "80/80 [==============================] - 0s 365us/sample - loss: 2.8364 - val_loss: 3.6490\n",
      "Epoch 526/1000\n",
      "80/80 [==============================] - 0s 409us/sample - loss: 2.5994 - val_loss: 3.1377\n",
      "Epoch 527/1000\n",
      "80/80 [==============================] - 0s 357us/sample - loss: 2.9313 - val_loss: 5.0775\n",
      "Epoch 528/1000\n",
      "80/80 [==============================] - 0s 339us/sample - loss: 2.5315 - val_loss: 4.2806\n",
      "Epoch 529/1000\n",
      "80/80 [==============================] - 0s 308us/sample - loss: 2.5648 - val_loss: 2.8798\n",
      "Epoch 530/1000\n",
      "80/80 [==============================] - 0s 268us/sample - loss: 2.6755 - val_loss: 3.1335\n",
      "Epoch 531/1000\n",
      "80/80 [==============================] - 0s 314us/sample - loss: 2.4821 - val_loss: 3.9327\n",
      "Epoch 532/1000\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 2.7361 - val_loss: 5.1399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 533/1000\n",
      "80/80 [==============================] - 0s 525us/sample - loss: 2.6819 - val_loss: 2.7535\n",
      "Epoch 534/1000\n",
      "80/80 [==============================] - 0s 379us/sample - loss: 2.6139 - val_loss: 3.8209\n",
      "Epoch 535/1000\n",
      "80/80 [==============================] - 0s 376us/sample - loss: 2.4663 - val_loss: 5.2586\n",
      "Epoch 536/1000\n",
      "80/80 [==============================] - 0s 309us/sample - loss: 2.4577 - val_loss: 3.0834\n",
      "Epoch 537/1000\n",
      "80/80 [==============================] - 0s 520us/sample - loss: 2.4317 - val_loss: 2.9562\n",
      "Epoch 538/1000\n",
      "80/80 [==============================] - 0s 387us/sample - loss: 2.4987 - val_loss: 4.2907\n",
      "Epoch 539/1000\n",
      "80/80 [==============================] - 0s 383us/sample - loss: 2.4896 - val_loss: 3.8035\n",
      "Epoch 540/1000\n",
      "80/80 [==============================] - 0s 519us/sample - loss: 2.8757 - val_loss: 2.7730\n",
      "Epoch 541/1000\n",
      "80/80 [==============================] - 0s 390us/sample - loss: 2.6139 - val_loss: 4.6395\n",
      "Epoch 542/1000\n",
      "80/80 [==============================] - 0s 517us/sample - loss: 2.4774 - val_loss: 4.3819\n",
      "Epoch 543/1000\n",
      "80/80 [==============================] - 0s 533us/sample - loss: 3.1880 - val_loss: 2.9508\n",
      "Epoch 544/1000\n",
      "80/80 [==============================] - 0s 545us/sample - loss: 3.0972 - val_loss: 4.7364\n",
      "Epoch 545/1000\n",
      "80/80 [==============================] - 0s 486us/sample - loss: 3.1404 - val_loss: 3.3882\n",
      "Epoch 546/1000\n",
      "80/80 [==============================] - 0s 402us/sample - loss: 2.9373 - val_loss: 5.1407\n",
      "Epoch 547/1000\n",
      "80/80 [==============================] - 0s 474us/sample - loss: 2.4723 - val_loss: 4.4076\n",
      "Epoch 548/1000\n",
      "80/80 [==============================] - 0s 543us/sample - loss: 2.7484 - val_loss: 3.2392\n",
      "Epoch 549/1000\n",
      "80/80 [==============================] - 0s 436us/sample - loss: 2.4121 - val_loss: 4.0638\n",
      "Epoch 550/1000\n",
      "80/80 [==============================] - 0s 420us/sample - loss: 2.3127 - val_loss: 4.3571\n",
      "Epoch 551/1000\n",
      "80/80 [==============================] - 0s 530us/sample - loss: 2.5584 - val_loss: 4.1610\n",
      "Epoch 552/1000\n",
      "80/80 [==============================] - 0s 541us/sample - loss: 2.3967 - val_loss: 3.0059\n",
      "Epoch 553/1000\n",
      "80/80 [==============================] - 0s 400us/sample - loss: 2.5225 - val_loss: 3.5926\n",
      "Epoch 554/1000\n",
      "80/80 [==============================] - 0s 472us/sample - loss: 2.4400 - val_loss: 5.8598\n",
      "Epoch 555/1000\n",
      "80/80 [==============================] - 0s 432us/sample - loss: 2.4541 - val_loss: 3.2330\n",
      "Epoch 556/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 1.980 - 0s 415us/sample - loss: 2.5860 - val_loss: 2.6666\n",
      "Epoch 557/1000\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 2.5655 - val_loss: 5.0276\n",
      "Epoch 558/1000\n",
      "80/80 [==============================] - 0s 448us/sample - loss: 2.6534 - val_loss: 4.6531\n",
      "Epoch 559/1000\n",
      "80/80 [==============================] - 0s 482us/sample - loss: 2.5462 - val_loss: 2.4894\n",
      "Epoch 560/1000\n",
      "80/80 [==============================] - 0s 393us/sample - loss: 2.4369 - val_loss: 4.4276\n",
      "Epoch 561/1000\n",
      "80/80 [==============================] - 0s 277us/sample - loss: 2.7036 - val_loss: 4.5332\n",
      "Epoch 562/1000\n",
      "80/80 [==============================] - 0s 374us/sample - loss: 2.3416 - val_loss: 3.1104\n",
      "Epoch 563/1000\n",
      "80/80 [==============================] - 0s 370us/sample - loss: 2.8536 - val_loss: 3.8839\n",
      "Epoch 564/1000\n",
      "80/80 [==============================] - 0s 423us/sample - loss: 2.3483 - val_loss: 4.1213\n",
      "Epoch 565/1000\n",
      "80/80 [==============================] - 0s 301us/sample - loss: 2.5212 - val_loss: 3.8843\n",
      "Epoch 566/1000\n",
      "80/80 [==============================] - 0s 404us/sample - loss: 2.2968 - val_loss: 3.0939\n",
      "Epoch 567/1000\n",
      "80/80 [==============================] - 0s 468us/sample - loss: 2.3280 - val_loss: 3.7891\n",
      "Epoch 568/1000\n",
      "80/80 [==============================] - 0s 369us/sample - loss: 2.2709 - val_loss: 3.6741\n",
      "Epoch 569/1000\n",
      "80/80 [==============================] - 0s 432us/sample - loss: 2.2808 - val_loss: 3.3554\n",
      "Epoch 570/1000\n",
      "80/80 [==============================] - 0s 377us/sample - loss: 2.2942 - val_loss: 3.3026\n",
      "Epoch 571/1000\n",
      "80/80 [==============================] - 0s 508us/sample - loss: 2.2731 - val_loss: 3.7149\n",
      "Epoch 572/1000\n",
      "80/80 [==============================] - 0s 600us/sample - loss: 2.3696 - val_loss: 3.6332\n",
      "Epoch 573/1000\n",
      "80/80 [==============================] - 0s 544us/sample - loss: 2.4455 - val_loss: 3.5186\n",
      "Epoch 574/1000\n",
      "80/80 [==============================] - 0s 341us/sample - loss: 2.3687 - val_loss: 3.4354\n",
      "Epoch 575/1000\n",
      "80/80 [==============================] - 0s 263us/sample - loss: 2.5587 - val_loss: 3.3899\n",
      "Epoch 576/1000\n",
      "80/80 [==============================] - 0s 439us/sample - loss: 2.3499 - val_loss: 3.7168\n",
      "Epoch 577/1000\n",
      "80/80 [==============================] - 0s 421us/sample - loss: 2.3763 - val_loss: 4.5253\n",
      "Epoch 578/1000\n",
      "80/80 [==============================] - 0s 320us/sample - loss: 2.7300 - val_loss: 4.1066\n",
      "Epoch 579/1000\n",
      "80/80 [==============================] - 0s 378us/sample - loss: 2.4558 - val_loss: 3.8907\n",
      "Epoch 580/1000\n",
      "80/80 [==============================] - 0s 380us/sample - loss: 2.7545 - val_loss: 3.5821\n",
      "Epoch 581/1000\n",
      "80/80 [==============================] - 0s 507us/sample - loss: 2.7761 - val_loss: 3.3839\n",
      "Epoch 582/1000\n",
      "80/80 [==============================] - 0s 328us/sample - loss: 2.5273 - val_loss: 4.7517\n",
      "Epoch 583/1000\n",
      "80/80 [==============================] - 0s 455us/sample - loss: 3.2004 - val_loss: 2.9289\n",
      "Epoch 584/1000\n",
      "80/80 [==============================] - 0s 415us/sample - loss: 2.6245 - val_loss: 4.8485\n",
      "Epoch 585/1000\n",
      "80/80 [==============================] - 0s 396us/sample - loss: 3.0851 - val_loss: 2.8285\n",
      "Epoch 586/1000\n",
      "80/80 [==============================] - 0s 389us/sample - loss: 3.9299 - val_loss: 4.9493\n",
      "Epoch 587/1000\n",
      "80/80 [==============================] - 0s 543us/sample - loss: 2.6173 - val_loss: 5.8692\n",
      "Epoch 588/1000\n",
      "80/80 [==============================] - 0s 678us/sample - loss: 4.1215 - val_loss: 2.1877\n",
      "Epoch 589/1000\n",
      "80/80 [==============================] - 0s 313us/sample - loss: 2.9177 - val_loss: 4.7776\n",
      "Epoch 590/1000\n",
      "80/80 [==============================] - 0s 354us/sample - loss: 2.5112 - val_loss: 4.5451\n",
      "Epoch 591/1000\n",
      "80/80 [==============================] - 0s 463us/sample - loss: 2.6602 - val_loss: 2.6701\n",
      "Epoch 592/1000\n",
      "80/80 [==============================] - 0s 314us/sample - loss: 2.9897 - val_loss: 3.2986\n",
      "Epoch 593/1000\n",
      "80/80 [==============================] - 0s 515us/sample - loss: 2.4139 - val_loss: 4.9975\n",
      "Epoch 594/1000\n",
      "80/80 [==============================] - 0s 308us/sample - loss: 2.4120 - val_loss: 2.5726\n",
      "Epoch 595/1000\n",
      "80/80 [==============================] - 0s 354us/sample - loss: 2.4284 - val_loss: 3.2826\n",
      "Epoch 596/1000\n",
      "80/80 [==============================] - 0s 323us/sample - loss: 2.1687 - val_loss: 4.9147\n",
      "Epoch 597/1000\n",
      "80/80 [==============================] - 0s 293us/sample - loss: 2.4221 - val_loss: 3.9681\n",
      "Epoch 598/1000\n",
      "80/80 [==============================] - 0s 294us/sample - loss: 2.2311 - val_loss: 3.5266\n",
      "Epoch 599/1000\n",
      "80/80 [==============================] - 0s 442us/sample - loss: 2.2906 - val_loss: 4.1241\n",
      "Epoch 600/1000\n",
      "80/80 [==============================] - 0s 382us/sample - loss: 2.2976 - val_loss: 4.4061\n",
      "Epoch 601/1000\n",
      "80/80 [==============================] - 0s 264us/sample - loss: 2.4562 - val_loss: 3.6401\n",
      "Epoch 602/1000\n",
      "80/80 [==============================] - 0s 381us/sample - loss: 2.2243 - val_loss: 3.5709\n",
      "Epoch 603/1000\n",
      "80/80 [==============================] - 0s 385us/sample - loss: 2.3683 - val_loss: 4.1210\n",
      "Epoch 604/1000\n",
      "80/80 [==============================] - 0s 419us/sample - loss: 2.4872 - val_loss: 2.9004\n",
      "Epoch 605/1000\n",
      "80/80 [==============================] - 0s 280us/sample - loss: 2.4391 - val_loss: 4.2214\n",
      "Epoch 606/1000\n",
      "80/80 [==============================] - 0s 461us/sample - loss: 2.1691 - val_loss: 2.8519\n",
      "Epoch 607/1000\n",
      "80/80 [==============================] - 0s 284us/sample - loss: 2.2922 - val_loss: 3.5247\n",
      "Epoch 608/1000\n",
      "80/80 [==============================] - 0s 296us/sample - loss: 2.2821 - val_loss: 3.5193\n",
      "Epoch 609/1000\n",
      "80/80 [==============================] - 0s 419us/sample - loss: 2.3897 - val_loss: 2.7490\n",
      "Epoch 610/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 398us/sample - loss: 2.2491 - val_loss: 4.8849\n",
      "Epoch 611/1000\n",
      "80/80 [==============================] - 0s 303us/sample - loss: 2.5787 - val_loss: 2.9976\n",
      "Epoch 612/1000\n",
      "80/80 [==============================] - 0s 253us/sample - loss: 2.3872 - val_loss: 2.3879\n",
      "Epoch 613/1000\n",
      "80/80 [==============================] - 0s 308us/sample - loss: 2.9565 - val_loss: 5.8851\n",
      "Epoch 614/1000\n",
      "80/80 [==============================] - 0s 394us/sample - loss: 3.5193 - val_loss: 4.5696\n",
      "Epoch 615/1000\n",
      "80/80 [==============================] - 0s 448us/sample - loss: 3.0265 - val_loss: 3.2579\n",
      "Epoch 616/1000\n",
      "80/80 [==============================] - 0s 395us/sample - loss: 2.8616 - val_loss: 5.3574\n",
      "Epoch 617/1000\n",
      "80/80 [==============================] - 0s 594us/sample - loss: 2.4458 - val_loss: 3.0773\n",
      "Epoch 618/1000\n",
      "80/80 [==============================] - 0s 362us/sample - loss: 2.2842 - val_loss: 2.8802\n",
      "Epoch 619/1000\n",
      "80/80 [==============================] - 0s 513us/sample - loss: 2.3446 - val_loss: 5.6968\n",
      "Epoch 620/1000\n",
      "80/80 [==============================] - 0s 591us/sample - loss: 2.8294 - val_loss: 3.8610\n",
      "Epoch 621/1000\n",
      "80/80 [==============================] - 0s 356us/sample - loss: 2.2781 - val_loss: 2.2359\n",
      "Epoch 622/1000\n",
      "80/80 [==============================] - 0s 351us/sample - loss: 2.8212 - val_loss: 3.7462\n",
      "Epoch 623/1000\n",
      "80/80 [==============================] - 0s 367us/sample - loss: 2.3048 - val_loss: 4.3608\n",
      "Epoch 624/1000\n",
      "80/80 [==============================] - 0s 486us/sample - loss: 2.3706 - val_loss: 2.8303\n",
      "Epoch 625/1000\n",
      "80/80 [==============================] - 0s 428us/sample - loss: 2.3836 - val_loss: 2.6914\n",
      "Epoch 626/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.499 - 0s 415us/sample - loss: 2.2194 - val_loss: 4.3760\n",
      "Epoch 627/1000\n",
      "80/80 [==============================] - 0s 394us/sample - loss: 2.4557 - val_loss: 3.7502\n",
      "Epoch 628/1000\n",
      "80/80 [==============================] - 0s 372us/sample - loss: 2.3914 - val_loss: 3.7443\n",
      "Epoch 629/1000\n",
      "80/80 [==============================] - 0s 384us/sample - loss: 2.4616 - val_loss: 2.2669\n",
      "Epoch 630/1000\n",
      "80/80 [==============================] - 0s 311us/sample - loss: 2.5844 - val_loss: 5.4945\n",
      "Epoch 631/1000\n",
      "80/80 [==============================] - 0s 478us/sample - loss: 2.5984 - val_loss: 4.2098\n",
      "Epoch 632/1000\n",
      "80/80 [==============================] - 0s 376us/sample - loss: 2.2101 - val_loss: 1.8389\n",
      "Epoch 633/1000\n",
      "80/80 [==============================] - 0s 487us/sample - loss: 2.5719 - val_loss: 3.1618\n",
      "Epoch 634/1000\n",
      "80/80 [==============================] - 0s 625us/sample - loss: 2.0544 - val_loss: 4.0658\n",
      "Epoch 635/1000\n",
      "80/80 [==============================] - 0s 715us/sample - loss: 2.2005 - val_loss: 3.4481\n",
      "Epoch 636/1000\n",
      "80/80 [==============================] - 0s 417us/sample - loss: 2.1997 - val_loss: 2.7432\n",
      "Epoch 637/1000\n",
      "80/80 [==============================] - 0s 388us/sample - loss: 2.1961 - val_loss: 3.3676\n",
      "Epoch 638/1000\n",
      "80/80 [==============================] - 0s 353us/sample - loss: 2.2255 - val_loss: 4.5246\n",
      "Epoch 639/1000\n",
      "80/80 [==============================] - 0s 393us/sample - loss: 2.1371 - val_loss: 3.1422\n",
      "Epoch 640/1000\n",
      "80/80 [==============================] - 0s 446us/sample - loss: 2.3332 - val_loss: 2.9528\n",
      "Epoch 641/1000\n",
      "80/80 [==============================] - 0s 462us/sample - loss: 2.1112 - val_loss: 4.3823\n",
      "Epoch 642/1000\n",
      "80/80 [==============================] - 0s 438us/sample - loss: 2.2556 - val_loss: 3.2470\n",
      "Epoch 643/1000\n",
      "80/80 [==============================] - 0s 458us/sample - loss: 2.2711 - val_loss: 2.8643\n",
      "Epoch 644/1000\n",
      "80/80 [==============================] - 0s 418us/sample - loss: 2.1087 - val_loss: 3.6769\n",
      "Epoch 645/1000\n",
      "80/80 [==============================] - 0s 576us/sample - loss: 2.1806 - val_loss: 3.0865\n",
      "Epoch 646/1000\n",
      "80/80 [==============================] - 0s 440us/sample - loss: 2.1546 - val_loss: 2.5058\n",
      "Epoch 647/1000\n",
      "80/80 [==============================] - 0s 512us/sample - loss: 2.0947 - val_loss: 4.1078\n",
      "Epoch 648/1000\n",
      "80/80 [==============================] - 0s 355us/sample - loss: 2.3088 - val_loss: 3.7040\n",
      "Epoch 649/1000\n",
      "80/80 [==============================] - 0s 464us/sample - loss: 2.0981 - val_loss: 3.1204\n",
      "Epoch 650/1000\n",
      "80/80 [==============================] - 0s 356us/sample - loss: 2.2894 - val_loss: 2.3839\n",
      "Epoch 651/1000\n",
      "80/80 [==============================] - 0s 402us/sample - loss: 2.2513 - val_loss: 4.1452\n",
      "Epoch 652/1000\n",
      "80/80 [==============================] - 0s 424us/sample - loss: 2.1107 - val_loss: 3.5752\n",
      "Epoch 653/1000\n",
      "80/80 [==============================] - 0s 359us/sample - loss: 2.1053 - val_loss: 2.1267\n",
      "Epoch 654/1000\n",
      "80/80 [==============================] - 0s 309us/sample - loss: 2.3104 - val_loss: 3.7523\n",
      "Epoch 655/1000\n",
      "80/80 [==============================] - 0s 392us/sample - loss: 2.5463 - val_loss: 6.4375\n",
      "Epoch 656/1000\n",
      "80/80 [==============================] - 0s 503us/sample - loss: 2.6754 - val_loss: 1.5537\n",
      "Epoch 657/1000\n",
      "80/80 [==============================] - 0s 423us/sample - loss: 2.9360 - val_loss: 2.9005\n",
      "Epoch 658/1000\n",
      "80/80 [==============================] - 0s 359us/sample - loss: 2.8213 - val_loss: 6.5234\n",
      "Epoch 659/1000\n",
      "80/80 [==============================] - 0s 311us/sample - loss: 3.4031 - val_loss: 2.9403\n",
      "Epoch 660/1000\n",
      "80/80 [==============================] - 0s 387us/sample - loss: 2.0801 - val_loss: 4.0137\n",
      "Epoch 661/1000\n",
      "80/80 [==============================] - 0s 599us/sample - loss: 2.4009 - val_loss: 4.2853\n",
      "Epoch 662/1000\n",
      "80/80 [==============================] - 0s 353us/sample - loss: 2.0274 - val_loss: 3.0357\n",
      "Epoch 663/1000\n",
      "80/80 [==============================] - 0s 402us/sample - loss: 2.5409 - val_loss: 2.9887\n",
      "Epoch 664/1000\n",
      "80/80 [==============================] - 0s 614us/sample - loss: 2.2913 - val_loss: 4.7844\n",
      "Epoch 665/1000\n",
      "80/80 [==============================] - 0s 376us/sample - loss: 2.4244 - val_loss: 3.2686\n",
      "Epoch 666/1000\n",
      "80/80 [==============================] - 0s 378us/sample - loss: 2.1622 - val_loss: 3.4491\n",
      "Epoch 667/1000\n",
      "80/80 [==============================] - 0s 290us/sample - loss: 2.3489 - val_loss: 3.7020\n",
      "Epoch 668/1000\n",
      "80/80 [==============================] - 0s 388us/sample - loss: 2.3425 - val_loss: 2.6289\n",
      "Epoch 669/1000\n",
      "80/80 [==============================] - 0s 284us/sample - loss: 2.2177 - val_loss: 2.8014\n",
      "Epoch 670/1000\n",
      "80/80 [==============================] - 0s 355us/sample - loss: 2.2574 - val_loss: 4.3790\n",
      "Epoch 671/1000\n",
      "80/80 [==============================] - 0s 376us/sample - loss: 2.6014 - val_loss: 2.8440\n",
      "Epoch 672/1000\n",
      "80/80 [==============================] - 0s 361us/sample - loss: 3.0039 - val_loss: 2.7413\n",
      "Epoch 673/1000\n",
      "80/80 [==============================] - 0s 470us/sample - loss: 2.0582 - val_loss: 7.8148\n",
      "Epoch 674/1000\n",
      "80/80 [==============================] - 0s 362us/sample - loss: 3.0152 - val_loss: 2.4304\n",
      "Epoch 675/1000\n",
      "80/80 [==============================] - 0s 313us/sample - loss: 2.7783 - val_loss: 2.4151\n",
      "Epoch 676/1000\n",
      "80/80 [==============================] - 0s 412us/sample - loss: 2.0550 - val_loss: 5.7678\n",
      "Epoch 677/1000\n",
      "80/80 [==============================] - 0s 455us/sample - loss: 2.6166 - val_loss: 2.6675\n",
      "Epoch 678/1000\n",
      "80/80 [==============================] - 0s 494us/sample - loss: 2.9557 - val_loss: 2.7658\n",
      "Epoch 679/1000\n",
      "80/80 [==============================] - 0s 320us/sample - loss: 2.4251 - val_loss: 7.7313\n",
      "Epoch 680/1000\n",
      "80/80 [==============================] - 0s 446us/sample - loss: 2.5862 - val_loss: 1.7975\n",
      "Epoch 681/1000\n",
      "80/80 [==============================] - 0s 268us/sample - loss: 2.7370 - val_loss: 2.3833\n",
      "Epoch 682/1000\n",
      "80/80 [==============================] - 0s 459us/sample - loss: 2.0724 - val_loss: 5.3307\n",
      "Epoch 683/1000\n",
      "80/80 [==============================] - 0s 335us/sample - loss: 2.4232 - val_loss: 2.6922\n",
      "Epoch 684/1000\n",
      "80/80 [==============================] - 0s 281us/sample - loss: 2.1884 - val_loss: 2.4924\n",
      "Epoch 685/1000\n",
      "80/80 [==============================] - 0s 294us/sample - loss: 2.2101 - val_loss: 4.5573\n",
      "Epoch 686/1000\n",
      "80/80 [==============================] - 0s 412us/sample - loss: 2.2799 - val_loss: 2.8359\n",
      "Epoch 687/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 365us/sample - loss: 2.0449 - val_loss: 2.8294\n",
      "Epoch 688/1000\n",
      "80/80 [==============================] - 0s 376us/sample - loss: 2.1768 - val_loss: 3.0962\n",
      "Epoch 689/1000\n",
      "80/80 [==============================] - 0s 385us/sample - loss: 2.0207 - val_loss: 3.8171\n",
      "Epoch 690/1000\n",
      "80/80 [==============================] - 0s 324us/sample - loss: 2.2076 - val_loss: 3.3825\n",
      "Epoch 691/1000\n",
      "80/80 [==============================] - 0s 521us/sample - loss: 1.9845 - val_loss: 2.3449\n",
      "Epoch 692/1000\n",
      "80/80 [==============================] - 0s 385us/sample - loss: 2.2938 - val_loss: 3.1174\n",
      "Epoch 693/1000\n",
      "80/80 [==============================] - 0s 528us/sample - loss: 2.0098 - val_loss: 3.4730\n",
      "Epoch 694/1000\n",
      "80/80 [==============================] - 0s 463us/sample - loss: 2.1719 - val_loss: 2.5457\n",
      "Epoch 695/1000\n",
      "80/80 [==============================] - 0s 775us/sample - loss: 2.4999 - val_loss: 2.5315\n",
      "Epoch 696/1000\n",
      "80/80 [==============================] - 0s 477us/sample - loss: 2.1772 - val_loss: 5.9480\n",
      "Epoch 697/1000\n",
      "80/80 [==============================] - 0s 345us/sample - loss: 2.5168 - val_loss: 2.7882\n",
      "Epoch 698/1000\n",
      "80/80 [==============================] - 0s 433us/sample - loss: 2.0244 - val_loss: 2.0473\n",
      "Epoch 699/1000\n",
      "80/80 [==============================] - 0s 294us/sample - loss: 2.3921 - val_loss: 3.5738\n",
      "Epoch 700/1000\n",
      "80/80 [==============================] - 0s 355us/sample - loss: 2.3161 - val_loss: 4.4017\n",
      "Epoch 701/1000\n",
      "80/80 [==============================] - 0s 485us/sample - loss: 2.1089 - val_loss: 3.2139\n",
      "Epoch 702/1000\n",
      "80/80 [==============================] - 0s 378us/sample - loss: 2.0738 - val_loss: 2.9910\n",
      "Epoch 703/1000\n",
      "80/80 [==============================] - 0s 465us/sample - loss: 2.2440 - val_loss: 3.2616\n",
      "Epoch 704/1000\n",
      "80/80 [==============================] - 0s 315us/sample - loss: 2.2045 - val_loss: 3.1555\n",
      "Epoch 705/1000\n",
      "80/80 [==============================] - 0s 381us/sample - loss: 2.1616 - val_loss: 2.3630\n",
      "Epoch 706/1000\n",
      "80/80 [==============================] - 0s 322us/sample - loss: 2.0832 - val_loss: 3.5721\n",
      "Epoch 707/1000\n",
      "80/80 [==============================] - 0s 352us/sample - loss: 2.3587 - val_loss: 3.1338\n",
      "Epoch 708/1000\n",
      "80/80 [==============================] - 0s 514us/sample - loss: 2.5650 - val_loss: 1.8283\n",
      "Epoch 709/1000\n",
      "80/80 [==============================] - 0s 374us/sample - loss: 2.7589 - val_loss: 3.6881\n",
      "Epoch 710/1000\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 2.3048 - val_loss: 3.7374\n",
      "Epoch 711/1000\n",
      "80/80 [==============================] - 0s 449us/sample - loss: 2.2902 - val_loss: 3.3882\n",
      "Epoch 712/1000\n",
      "80/80 [==============================] - 0s 467us/sample - loss: 2.2174 - val_loss: 2.1846\n",
      "Epoch 713/1000\n",
      "80/80 [==============================] - 0s 394us/sample - loss: 2.2108 - val_loss: 3.5829\n",
      "Epoch 714/1000\n",
      "80/80 [==============================] - 0s 458us/sample - loss: 2.1063 - val_loss: 4.5875\n",
      "Epoch 715/1000\n",
      "80/80 [==============================] - 0s 350us/sample - loss: 2.4457 - val_loss: 3.6312\n",
      "Epoch 716/1000\n",
      "80/80 [==============================] - 0s 381us/sample - loss: 1.8391 - val_loss: 4.2263\n",
      "Epoch 717/1000\n",
      "80/80 [==============================] - 0s 399us/sample - loss: 2.7464 - val_loss: 2.0290\n",
      "Epoch 718/1000\n",
      "80/80 [==============================] - 0s 487us/sample - loss: 2.6538 - val_loss: 4.7478\n",
      "Epoch 719/1000\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 2.9159 - val_loss: 5.6873\n",
      "Epoch 720/1000\n",
      "80/80 [==============================] - 0s 363us/sample - loss: 2.9699 - val_loss: 1.5343\n",
      "Epoch 721/1000\n",
      "80/80 [==============================] - 0s 409us/sample - loss: 3.7140 - val_loss: 5.1889\n",
      "Epoch 722/1000\n",
      "80/80 [==============================] - 0s 454us/sample - loss: 3.1799 - val_loss: 8.1917\n",
      "Epoch 723/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.944 - 0s 615us/sample - loss: 2.9388 - val_loss: 1.4752\n",
      "Epoch 724/1000\n",
      "80/80 [==============================] - 0s 769us/sample - loss: 2.7164 - val_loss: 2.0685\n",
      "Epoch 725/1000\n",
      "80/80 [==============================] - 0s 404us/sample - loss: 2.9063 - val_loss: 7.1777\n",
      "Epoch 726/1000\n",
      "80/80 [==============================] - 0s 316us/sample - loss: 2.6769 - val_loss: 2.5495\n",
      "Epoch 727/1000\n",
      "80/80 [==============================] - 0s 344us/sample - loss: 2.4601 - val_loss: 1.4685\n",
      "Epoch 728/1000\n",
      "80/80 [==============================] - 0s 399us/sample - loss: 2.3973 - val_loss: 5.5545\n",
      "Epoch 729/1000\n",
      "80/80 [==============================] - 0s 411us/sample - loss: 2.7048 - val_loss: 5.0023\n",
      "Epoch 730/1000\n",
      "80/80 [==============================] - 0s 352us/sample - loss: 2.6284 - val_loss: 1.5548\n",
      "Epoch 731/1000\n",
      "80/80 [==============================] - 0s 307us/sample - loss: 2.7755 - val_loss: 4.6903\n",
      "Epoch 732/1000\n",
      "80/80 [==============================] - 0s 441us/sample - loss: 2.5700 - val_loss: 4.9621\n",
      "Epoch 733/1000\n",
      "80/80 [==============================] - 0s 409us/sample - loss: 2.2877 - val_loss: 2.3992\n",
      "Epoch 734/1000\n",
      "80/80 [==============================] - 0s 354us/sample - loss: 2.3861 - val_loss: 4.0689\n",
      "Epoch 735/1000\n",
      "80/80 [==============================] - 0s 418us/sample - loss: 2.3368 - val_loss: 4.7501\n",
      "Epoch 736/1000\n",
      "80/80 [==============================] - 0s 370us/sample - loss: 2.3073 - val_loss: 3.0809\n",
      "Epoch 737/1000\n",
      "80/80 [==============================] - 0s 442us/sample - loss: 2.0812 - val_loss: 3.5786\n",
      "Epoch 738/1000\n",
      "80/80 [==============================] - 0s 539us/sample - loss: 2.1166 - val_loss: 2.7970\n",
      "Epoch 739/1000\n",
      "80/80 [==============================] - 0s 519us/sample - loss: 2.2945 - val_loss: 3.5387\n",
      "Epoch 740/1000\n",
      "80/80 [==============================] - 0s 463us/sample - loss: 2.1968 - val_loss: 4.0839\n",
      "Epoch 741/1000\n",
      "80/80 [==============================] - 0s 478us/sample - loss: 2.2934 - val_loss: 2.6599\n",
      "Epoch 742/1000\n",
      "80/80 [==============================] - 0s 386us/sample - loss: 2.2433 - val_loss: 2.3898\n",
      "Epoch 743/1000\n",
      "80/80 [==============================] - 0s 337us/sample - loss: 2.0102 - val_loss: 4.4631\n",
      "Epoch 744/1000\n",
      "80/80 [==============================] - 0s 336us/sample - loss: 2.2349 - val_loss: 2.7734\n",
      "Epoch 745/1000\n",
      "80/80 [==============================] - 0s 351us/sample - loss: 2.0104 - val_loss: 2.5225\n",
      "Epoch 746/1000\n",
      "80/80 [==============================] - 0s 365us/sample - loss: 2.0963 - val_loss: 3.9819\n",
      "Epoch 747/1000\n",
      "80/80 [==============================] - 0s 529us/sample - loss: 2.1071 - val_loss: 2.9406\n",
      "Epoch 748/1000\n",
      "80/80 [==============================] - 0s 459us/sample - loss: 2.1773 - val_loss: 3.1690\n",
      "Epoch 749/1000\n",
      "80/80 [==============================] - 0s 351us/sample - loss: 1.9417 - val_loss: 2.3289\n",
      "Epoch 750/1000\n",
      "80/80 [==============================] - 0s 359us/sample - loss: 2.0783 - val_loss: 3.2015\n",
      "Epoch 751/1000\n",
      "80/80 [==============================] - 0s 318us/sample - loss: 2.0476 - val_loss: 4.4718\n",
      "Epoch 752/1000\n",
      "80/80 [==============================] - 0s 267us/sample - loss: 2.0535 - val_loss: 3.0463\n",
      "Epoch 753/1000\n",
      "80/80 [==============================] - 0s 519us/sample - loss: 1.9812 - val_loss: 2.2882\n",
      "Epoch 754/1000\n",
      "80/80 [==============================] - 0s 423us/sample - loss: 2.2972 - val_loss: 2.9509\n",
      "Epoch 755/1000\n",
      "80/80 [==============================] - 0s 325us/sample - loss: 1.8832 - val_loss: 5.7143\n",
      "Epoch 756/1000\n",
      "80/80 [==============================] - 0s 510us/sample - loss: 2.3663 - val_loss: 2.9897\n",
      "Epoch 757/1000\n",
      "80/80 [==============================] - 0s 426us/sample - loss: 2.0658 - val_loss: 2.6225\n",
      "Epoch 758/1000\n",
      "80/80 [==============================] - 0s 421us/sample - loss: 1.9276 - val_loss: 3.2182\n",
      "Epoch 759/1000\n",
      "80/80 [==============================] - 0s 320us/sample - loss: 2.0740 - val_loss: 3.5572\n",
      "Epoch 760/1000\n",
      "80/80 [==============================] - 0s 290us/sample - loss: 1.9704 - val_loss: 2.3227\n",
      "Epoch 761/1000\n",
      "80/80 [==============================] - 0s 304us/sample - loss: 2.1684 - val_loss: 3.2970\n",
      "Epoch 762/1000\n",
      "80/80 [==============================] - 0s 422us/sample - loss: 2.2498 - val_loss: 4.9086\n",
      "Epoch 763/1000\n",
      "80/80 [==============================] - 0s 331us/sample - loss: 2.2378 - val_loss: 1.7384\n",
      "Epoch 764/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 307us/sample - loss: 2.1542 - val_loss: 3.5289\n",
      "Epoch 765/1000\n",
      "80/80 [==============================] - 0s 282us/sample - loss: 2.5966 - val_loss: 3.9354\n",
      "Epoch 766/1000\n",
      "80/80 [==============================] - 0s 452us/sample - loss: 2.4670 - val_loss: 3.4504\n",
      "Epoch 767/1000\n",
      "80/80 [==============================] - 0s 432us/sample - loss: 2.0691 - val_loss: 2.8449\n",
      "Epoch 768/1000\n",
      "80/80 [==============================] - 0s 436us/sample - loss: 2.7046 - val_loss: 3.2515\n",
      "Epoch 769/1000\n",
      "80/80 [==============================] - 0s 402us/sample - loss: 3.0967 - val_loss: 3.3609\n",
      "Epoch 770/1000\n",
      "80/80 [==============================] - 0s 469us/sample - loss: 3.0419 - val_loss: 2.8949\n",
      "Epoch 771/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 1.606 - 0s 506us/sample - loss: 2.6085 - val_loss: 3.6946\n",
      "Epoch 772/1000\n",
      "80/80 [==============================] - 0s 350us/sample - loss: 2.3469 - val_loss: 6.2059\n",
      "Epoch 773/1000\n",
      "80/80 [==============================] - 0s 382us/sample - loss: 2.3666 - val_loss: 2.2483\n",
      "Epoch 774/1000\n",
      "80/80 [==============================] - 0s 349us/sample - loss: 3.1887 - val_loss: 2.3888\n",
      "Epoch 775/1000\n",
      "80/80 [==============================] - 0s 405us/sample - loss: 2.2941 - val_loss: 6.7813\n",
      "Epoch 776/1000\n",
      "80/80 [==============================] - 0s 523us/sample - loss: 2.5919 - val_loss: 1.4221\n",
      "Epoch 777/1000\n",
      "80/80 [==============================] - 0s 388us/sample - loss: 2.5483 - val_loss: 2.5398\n",
      "Epoch 778/1000\n",
      "80/80 [==============================] - 0s 429us/sample - loss: 2.2909 - val_loss: 5.8223\n",
      "Epoch 779/1000\n",
      "80/80 [==============================] - 0s 469us/sample - loss: 2.5720 - val_loss: 2.3541\n",
      "Epoch 780/1000\n",
      "80/80 [==============================] - 0s 372us/sample - loss: 2.1201 - val_loss: 2.6254\n",
      "Epoch 781/1000\n",
      "80/80 [==============================] - 0s 404us/sample - loss: 2.1032 - val_loss: 3.8760\n",
      "Epoch 782/1000\n",
      "80/80 [==============================] - 0s 465us/sample - loss: 2.2086 - val_loss: 5.0873\n",
      "Epoch 783/1000\n",
      "80/80 [==============================] - 0s 733us/sample - loss: 2.1843 - val_loss: 2.9673\n",
      "Epoch 784/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.794 - 0s 380us/sample - loss: 2.1831 - val_loss: 2.5600\n",
      "Epoch 785/1000\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 2.3695 - val_loss: 3.1039\n",
      "Epoch 786/1000\n",
      "80/80 [==============================] - 0s 369us/sample - loss: 2.1608 - val_loss: 4.6664\n",
      "Epoch 787/1000\n",
      "80/80 [==============================] - 0s 410us/sample - loss: 2.1752 - val_loss: 2.1209\n",
      "Epoch 788/1000\n",
      "80/80 [==============================] - 0s 499us/sample - loss: 2.0507 - val_loss: 3.6223\n",
      "Epoch 789/1000\n",
      "80/80 [==============================] - 0s 357us/sample - loss: 2.3022 - val_loss: 4.1770\n",
      "Epoch 790/1000\n",
      "80/80 [==============================] - 0s 450us/sample - loss: 2.2232 - val_loss: 2.8524\n",
      "Epoch 791/1000\n",
      "80/80 [==============================] - 0s 326us/sample - loss: 2.2313 - val_loss: 3.0537\n",
      "Epoch 792/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 1.430 - 0s 293us/sample - loss: 1.9408 - val_loss: 3.0032\n",
      "Epoch 793/1000\n",
      "80/80 [==============================] - 0s 366us/sample - loss: 2.0462 - val_loss: 2.3561\n",
      "Epoch 794/1000\n",
      "80/80 [==============================] - 0s 316us/sample - loss: 3.4492 - val_loss: 3.7313\n",
      "Epoch 795/1000\n",
      "80/80 [==============================] - 0s 408us/sample - loss: 2.7194 - val_loss: 5.2939\n",
      "Epoch 796/1000\n",
      "80/80 [==============================] - 0s 455us/sample - loss: 3.1016 - val_loss: 2.2111\n",
      "Epoch 797/1000\n",
      "80/80 [==============================] - 0s 624us/sample - loss: 2.8769 - val_loss: 4.0956\n",
      "Epoch 798/1000\n",
      "80/80 [==============================] - 0s 498us/sample - loss: 3.1170 - val_loss: 3.8660\n",
      "Epoch 799/1000\n",
      "80/80 [==============================] - 0s 415us/sample - loss: 2.5971 - val_loss: 1.6096\n",
      "Epoch 800/1000\n",
      "80/80 [==============================] - 0s 317us/sample - loss: 2.6960 - val_loss: 6.7368\n",
      "Epoch 801/1000\n",
      "80/80 [==============================] - 0s 376us/sample - loss: 2.6864 - val_loss: 3.1741\n",
      "Epoch 802/1000\n",
      "80/80 [==============================] - 0s 395us/sample - loss: 2.2250 - val_loss: 1.9136\n",
      "Epoch 803/1000\n",
      "80/80 [==============================] - 0s 318us/sample - loss: 2.0548 - val_loss: 5.2308\n",
      "Epoch 804/1000\n",
      "80/80 [==============================] - 0s 297us/sample - loss: 2.4024 - val_loss: 3.6404\n",
      "Epoch 805/1000\n",
      "80/80 [==============================] - 0s 430us/sample - loss: 2.1281 - val_loss: 1.6213\n",
      "Epoch 806/1000\n",
      "80/80 [==============================] - 0s 303us/sample - loss: 2.3003 - val_loss: 4.1660\n",
      "Epoch 807/1000\n",
      "80/80 [==============================] - 0s 377us/sample - loss: 2.4620 - val_loss: 3.3977\n",
      "Epoch 808/1000\n",
      "80/80 [==============================] - 0s 283us/sample - loss: 2.0211 - val_loss: 1.5982\n",
      "Epoch 809/1000\n",
      "80/80 [==============================] - 0s 425us/sample - loss: 2.3611 - val_loss: 2.5716\n",
      "Epoch 810/1000\n",
      "80/80 [==============================] - 0s 309us/sample - loss: 2.5669 - val_loss: 3.2503\n",
      "Epoch 811/1000\n",
      "80/80 [==============================] - 0s 415us/sample - loss: 2.0934 - val_loss: 3.3505\n",
      "Epoch 812/1000\n",
      "80/80 [==============================] - 0s 368us/sample - loss: 2.0436 - val_loss: 1.8605\n",
      "Epoch 813/1000\n",
      "80/80 [==============================] - 0s 397us/sample - loss: 2.0499 - val_loss: 3.1287\n",
      "Epoch 814/1000\n",
      "80/80 [==============================] - 0s 508us/sample - loss: 2.0288 - val_loss: 3.7676\n",
      "Epoch 815/1000\n",
      "80/80 [==============================] - 0s 415us/sample - loss: 2.0676 - val_loss: 3.8268\n",
      "Epoch 816/1000\n",
      "80/80 [==============================] - 0s 297us/sample - loss: 2.0651 - val_loss: 3.0492\n",
      "Epoch 817/1000\n",
      "80/80 [==============================] - 0s 357us/sample - loss: 1.8823 - val_loss: 2.3663\n",
      "Epoch 818/1000\n",
      "80/80 [==============================] - 0s 299us/sample - loss: 2.1001 - val_loss: 3.4141\n",
      "Epoch 819/1000\n",
      "80/80 [==============================] - 0s 376us/sample - loss: 1.9698 - val_loss: 2.8051\n",
      "Epoch 820/1000\n",
      "80/80 [==============================] - 0s 369us/sample - loss: 1.9378 - val_loss: 2.4305\n",
      "Epoch 821/1000\n",
      "80/80 [==============================] - 0s 405us/sample - loss: 2.0471 - val_loss: 3.3102\n",
      "Epoch 822/1000\n",
      "80/80 [==============================] - 0s 423us/sample - loss: 2.1700 - val_loss: 4.0936\n",
      "Epoch 823/1000\n",
      "80/80 [==============================] - 0s 411us/sample - loss: 2.3307 - val_loss: 1.9654\n",
      "Epoch 824/1000\n",
      "80/80 [==============================] - 0s 433us/sample - loss: 2.5879 - val_loss: 1.2853\n",
      "Epoch 825/1000\n",
      "80/80 [==============================] - 0s 327us/sample - loss: 2.1643 - val_loss: 4.7062\n",
      "Epoch 826/1000\n",
      "80/80 [==============================] - 0s 314us/sample - loss: 2.8105 - val_loss: 4.3754\n",
      "Epoch 827/1000\n",
      "80/80 [==============================] - 0s 236us/sample - loss: 2.0784 - val_loss: 1.4252\n",
      "Epoch 828/1000\n",
      "80/80 [==============================] - 0s 276us/sample - loss: 2.6914 - val_loss: 3.4697\n",
      "Epoch 829/1000\n",
      "80/80 [==============================] - 0s 376us/sample - loss: 2.2143 - val_loss: 4.2957\n",
      "Epoch 830/1000\n",
      "80/80 [==============================] - 0s 332us/sample - loss: 2.3347 - val_loss: 3.8976\n",
      "Epoch 831/1000\n",
      "80/80 [==============================] - 0s 345us/sample - loss: 2.2206 - val_loss: 1.3472\n",
      "Epoch 832/1000\n",
      "80/80 [==============================] - 0s 396us/sample - loss: 2.3015 - val_loss: 3.8206\n",
      "Epoch 833/1000\n",
      "80/80 [==============================] - 0s 298us/sample - loss: 2.2989 - val_loss: 3.9821\n",
      "Epoch 834/1000\n",
      "80/80 [==============================] - 0s 417us/sample - loss: 1.9239 - val_loss: 1.6981\n",
      "Epoch 835/1000\n",
      "80/80 [==============================] - 0s 383us/sample - loss: 2.3511 - val_loss: 2.6792\n",
      "Epoch 836/1000\n",
      "80/80 [==============================] - 0s 314us/sample - loss: 1.9077 - val_loss: 4.0315\n",
      "Epoch 837/1000\n",
      "80/80 [==============================] - 0s 330us/sample - loss: 1.9489 - val_loss: 1.9845\n",
      "Epoch 838/1000\n",
      "80/80 [==============================] - 0s 368us/sample - loss: 2.0652 - val_loss: 1.8952\n",
      "Epoch 839/1000\n",
      "80/80 [==============================] - 0s 260us/sample - loss: 2.0628 - val_loss: 3.9567\n",
      "Epoch 840/1000\n",
      "80/80 [==============================] - 0s 280us/sample - loss: 1.9812 - val_loss: 3.1686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 841/1000\n",
      "80/80 [==============================] - 0s 331us/sample - loss: 1.9370 - val_loss: 2.7729\n",
      "Epoch 842/1000\n",
      "80/80 [==============================] - 0s 271us/sample - loss: 1.9478 - val_loss: 2.4361\n",
      "Epoch 843/1000\n",
      "80/80 [==============================] - 0s 378us/sample - loss: 2.0951 - val_loss: 2.5551\n",
      "Epoch 844/1000\n",
      "80/80 [==============================] - 0s 510us/sample - loss: 1.8606 - val_loss: 4.4927\n",
      "Epoch 845/1000\n",
      "80/80 [==============================] - 0s 521us/sample - loss: 2.2055 - val_loss: 4.0672\n",
      "Epoch 846/1000\n",
      "80/80 [==============================] - 0s 461us/sample - loss: 2.2052 - val_loss: 1.8915\n",
      "Epoch 847/1000\n",
      "80/80 [==============================] - 0s 534us/sample - loss: 2.3047 - val_loss: 3.1032\n",
      "Epoch 848/1000\n",
      "80/80 [==============================] - 0s 379us/sample - loss: 2.2307 - val_loss: 4.2585\n",
      "Epoch 849/1000\n",
      "80/80 [==============================] - 0s 360us/sample - loss: 2.2662 - val_loss: 2.3068\n",
      "Epoch 850/1000\n",
      "80/80 [==============================] - 0s 464us/sample - loss: 2.6892 - val_loss: 3.5142\n",
      "Epoch 851/1000\n",
      "80/80 [==============================] - 0s 376us/sample - loss: 3.1885 - val_loss: 3.1855\n",
      "Epoch 852/1000\n",
      "80/80 [==============================] - 0s 390us/sample - loss: 3.7340 - val_loss: 4.7529\n",
      "Epoch 853/1000\n",
      "80/80 [==============================] - 0s 392us/sample - loss: 2.2400 - val_loss: 2.0533\n",
      "Epoch 854/1000\n",
      "80/80 [==============================] - 0s 339us/sample - loss: 2.6533 - val_loss: 2.5749\n",
      "Epoch 855/1000\n",
      "80/80 [==============================] - 0s 379us/sample - loss: 2.5270 - val_loss: 4.5063\n",
      "Epoch 856/1000\n",
      "80/80 [==============================] - 0s 448us/sample - loss: 2.4286 - val_loss: 1.7237\n",
      "Epoch 857/1000\n",
      "80/80 [==============================] - 0s 422us/sample - loss: 2.1935 - val_loss: 4.0244\n",
      "Epoch 858/1000\n",
      "80/80 [==============================] - 0s 399us/sample - loss: 2.1045 - val_loss: 2.4917\n",
      "Epoch 859/1000\n",
      "80/80 [==============================] - 0s 396us/sample - loss: 2.8617 - val_loss: 1.7914\n",
      "Epoch 860/1000\n",
      "80/80 [==============================] - 0s 486us/sample - loss: 2.1272 - val_loss: 8.5395\n",
      "Epoch 861/1000\n",
      "80/80 [==============================] - 0s 543us/sample - loss: 3.1523 - val_loss: 1.8152\n",
      "Epoch 862/1000\n",
      "80/80 [==============================] - 0s 333us/sample - loss: 3.3389 - val_loss: 1.3798\n",
      "Epoch 863/1000\n",
      "80/80 [==============================] - 0s 366us/sample - loss: 2.0891 - val_loss: 6.5626\n",
      "Epoch 864/1000\n",
      "80/80 [==============================] - 0s 297us/sample - loss: 3.9792 - val_loss: 4.5340\n",
      "Epoch 865/1000\n",
      "80/80 [==============================] - 0s 405us/sample - loss: 2.4373 - val_loss: 1.3548\n",
      "Epoch 866/1000\n",
      "80/80 [==============================] - 0s 380us/sample - loss: 2.7231 - val_loss: 4.5279\n",
      "Epoch 867/1000\n",
      "80/80 [==============================] - 0s 359us/sample - loss: 2.9802 - val_loss: 6.5003\n",
      "Epoch 868/1000\n",
      "80/80 [==============================] - 0s 386us/sample - loss: 2.6488 - val_loss: 1.4020\n",
      "Epoch 869/1000\n",
      "80/80 [==============================] - 0s 347us/sample - loss: 2.2121 - val_loss: 3.3226\n",
      "Epoch 870/1000\n",
      "80/80 [==============================] - 0s 451us/sample - loss: 1.9769 - val_loss: 3.9413\n",
      "Epoch 871/1000\n",
      "80/80 [==============================] - 0s 370us/sample - loss: 2.0996 - val_loss: 2.2640\n",
      "Epoch 872/1000\n",
      "80/80 [==============================] - 0s 433us/sample - loss: 2.0091 - val_loss: 2.7007\n",
      "Epoch 873/1000\n",
      "80/80 [==============================] - 0s 404us/sample - loss: 1.8932 - val_loss: 4.0550\n",
      "Epoch 874/1000\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 2.1430 - val_loss: 2.9951\n",
      "Epoch 875/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 1.622 - 0s 412us/sample - loss: 2.0202 - val_loss: 2.8082\n",
      "Epoch 876/1000\n",
      "80/80 [==============================] - 0s 517us/sample - loss: 2.0844 - val_loss: 2.4400\n",
      "Epoch 877/1000\n",
      "80/80 [==============================] - 0s 485us/sample - loss: 1.9357 - val_loss: 3.2689\n",
      "Epoch 878/1000\n",
      "80/80 [==============================] - 0s 368us/sample - loss: 2.0774 - val_loss: 3.0024\n",
      "Epoch 879/1000\n",
      "80/80 [==============================] - 0s 378us/sample - loss: 1.9156 - val_loss: 3.4923\n",
      "Epoch 880/1000\n",
      "80/80 [==============================] - 0s 331us/sample - loss: 2.2965 - val_loss: 3.2606\n",
      "Epoch 881/1000\n",
      "80/80 [==============================] - 0s 414us/sample - loss: 1.9623 - val_loss: 2.7107\n",
      "Epoch 882/1000\n",
      "80/80 [==============================] - 0s 261us/sample - loss: 2.3983 - val_loss: 4.6500\n",
      "Epoch 883/1000\n",
      "80/80 [==============================] - 0s 361us/sample - loss: 1.9932 - val_loss: 1.4034\n",
      "Epoch 884/1000\n",
      "80/80 [==============================] - 0s 456us/sample - loss: 2.2612 - val_loss: 2.9095\n",
      "Epoch 885/1000\n",
      "80/80 [==============================] - 0s 320us/sample - loss: 2.0904 - val_loss: 4.2297\n",
      "Epoch 886/1000\n",
      "80/80 [==============================] - 0s 424us/sample - loss: 2.3414 - val_loss: 1.3968\n",
      "Epoch 887/1000\n",
      "80/80 [==============================] - 0s 383us/sample - loss: 2.8073 - val_loss: 4.0166\n",
      "Epoch 888/1000\n",
      "80/80 [==============================] - 0s 352us/sample - loss: 2.5688 - val_loss: 4.7190\n",
      "Epoch 889/1000\n",
      "80/80 [==============================] - 0s 364us/sample - loss: 2.7623 - val_loss: 1.7093\n",
      "Epoch 890/1000\n",
      "80/80 [==============================] - 0s 351us/sample - loss: 2.4395 - val_loss: 3.1269\n",
      "Epoch 891/1000\n",
      "80/80 [==============================] - 0s 434us/sample - loss: 2.9867 - val_loss: 5.2475\n",
      "Epoch 892/1000\n",
      "80/80 [==============================] - 0s 552us/sample - loss: 2.9777 - val_loss: 1.9016\n",
      "Epoch 893/1000\n",
      "80/80 [==============================] - 0s 374us/sample - loss: 2.6389 - val_loss: 3.4452\n",
      "Epoch 894/1000\n",
      "80/80 [==============================] - 0s 392us/sample - loss: 2.4794 - val_loss: 4.2935\n",
      "Epoch 895/1000\n",
      "80/80 [==============================] - 0s 459us/sample - loss: 2.7362 - val_loss: 2.8190\n",
      "Epoch 896/1000\n",
      "80/80 [==============================] - 0s 323us/sample - loss: 2.0529 - val_loss: 2.8381\n",
      "Epoch 897/1000\n",
      "80/80 [==============================] - 0s 347us/sample - loss: 2.0572 - val_loss: 3.5283\n",
      "Epoch 898/1000\n",
      "80/80 [==============================] - 0s 318us/sample - loss: 1.8938 - val_loss: 2.0736\n",
      "Epoch 899/1000\n",
      "80/80 [==============================] - 0s 464us/sample - loss: 2.1239 - val_loss: 2.4719\n",
      "Epoch 900/1000\n",
      "80/80 [==============================] - 0s 430us/sample - loss: 2.2095 - val_loss: 3.0265\n",
      "Epoch 901/1000\n",
      "80/80 [==============================] - 0s 294us/sample - loss: 1.8644 - val_loss: 2.1128\n",
      "Epoch 902/1000\n",
      "80/80 [==============================] - 0s 418us/sample - loss: 2.5631 - val_loss: 2.1072\n",
      "Epoch 903/1000\n",
      "80/80 [==============================] - 0s 274us/sample - loss: 3.0268 - val_loss: 5.8076\n",
      "Epoch 904/1000\n",
      "80/80 [==============================] - 0s 289us/sample - loss: 3.0633 - val_loss: 2.3676\n",
      "Epoch 905/1000\n",
      "80/80 [==============================] - 0s 288us/sample - loss: 4.1698 - val_loss: 1.7868\n",
      "Epoch 906/1000\n",
      "80/80 [==============================] - 0s 369us/sample - loss: 3.1089 - val_loss: 9.0809\n",
      "Epoch 907/1000\n",
      "80/80 [==============================] - 0s 303us/sample - loss: 3.4017 - val_loss: 1.2305\n",
      "Epoch 908/1000\n",
      "80/80 [==============================] - 0s 436us/sample - loss: 3.1037 - val_loss: 2.4553\n",
      "Epoch 909/1000\n",
      "80/80 [==============================] - 0s 424us/sample - loss: 3.4508 - val_loss: 4.8796\n",
      "Epoch 910/1000\n",
      "80/80 [==============================] - 0s 314us/sample - loss: 2.5338 - val_loss: 2.4855\n",
      "Epoch 911/1000\n",
      "80/80 [==============================] - 0s 472us/sample - loss: 2.3777 - val_loss: 2.3316\n",
      "Epoch 912/1000\n",
      "80/80 [==============================] - 0s 383us/sample - loss: 2.1847 - val_loss: 3.8370\n",
      "Epoch 913/1000\n",
      "80/80 [==============================] - 0s 320us/sample - loss: 2.6266 - val_loss: 3.4197\n",
      "Epoch 914/1000\n",
      "80/80 [==============================] - 0s 475us/sample - loss: 2.0727 - val_loss: 2.4513\n",
      "Epoch 915/1000\n",
      "80/80 [==============================] - 0s 292us/sample - loss: 2.7611 - val_loss: 2.9140\n",
      "Epoch 916/1000\n",
      "80/80 [==============================] - 0s 400us/sample - loss: 2.8606 - val_loss: 5.7202\n",
      "Epoch 917/1000\n",
      "80/80 [==============================] - 0s 282us/sample - loss: 2.3365 - val_loss: 2.5302\n",
      "Epoch 918/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 349us/sample - loss: 2.8018 - val_loss: 1.8231\n",
      "Epoch 919/1000\n",
      "80/80 [==============================] - 0s 495us/sample - loss: 1.8922 - val_loss: 4.1727\n",
      "Epoch 920/1000\n",
      "80/80 [==============================] - 0s 461us/sample - loss: 2.2923 - val_loss: 3.8142\n",
      "Epoch 921/1000\n",
      "80/80 [==============================] - 0s 256us/sample - loss: 2.1982 - val_loss: 2.1488\n",
      "Epoch 922/1000\n",
      "80/80 [==============================] - 0s 505us/sample - loss: 2.1015 - val_loss: 3.1680\n",
      "Epoch 923/1000\n",
      "80/80 [==============================] - 0s 532us/sample - loss: 2.2991 - val_loss: 5.1595\n",
      "Epoch 924/1000\n",
      "80/80 [==============================] - 0s 566us/sample - loss: 2.2952 - val_loss: 1.6756\n",
      "Epoch 925/1000\n",
      "80/80 [==============================] - 0s 499us/sample - loss: 2.2592 - val_loss: 3.2085\n",
      "Epoch 926/1000\n",
      "80/80 [==============================] - 0s 530us/sample - loss: 2.0921 - val_loss: 3.9279\n",
      "Epoch 927/1000\n",
      "80/80 [==============================] - 0s 468us/sample - loss: 1.8830 - val_loss: 2.6740\n",
      "Epoch 928/1000\n",
      "80/80 [==============================] - 0s 426us/sample - loss: 1.8462 - val_loss: 2.1463\n",
      "Epoch 929/1000\n",
      "80/80 [==============================] - 0s 410us/sample - loss: 1.9381 - val_loss: 2.4462\n",
      "Epoch 930/1000\n",
      "80/80 [==============================] - 0s 524us/sample - loss: 1.8763 - val_loss: 2.7916\n",
      "Epoch 931/1000\n",
      "80/80 [==============================] - 0s 439us/sample - loss: 2.0068 - val_loss: 2.3174\n",
      "Epoch 932/1000\n",
      "80/80 [==============================] - 0s 465us/sample - loss: 1.9696 - val_loss: 2.9626\n",
      "Epoch 933/1000\n",
      "80/80 [==============================] - 0s 355us/sample - loss: 2.4146 - val_loss: 2.1464\n",
      "Epoch 934/1000\n",
      "80/80 [==============================] - 0s 365us/sample - loss: 2.1257 - val_loss: 3.9743\n",
      "Epoch 935/1000\n",
      "80/80 [==============================] - 0s 456us/sample - loss: 2.0967 - val_loss: 1.9917\n",
      "Epoch 936/1000\n",
      "80/80 [==============================] - 0s 541us/sample - loss: 2.1342 - val_loss: 2.4927\n",
      "Epoch 937/1000\n",
      "80/80 [==============================] - 0s 616us/sample - loss: 2.1569 - val_loss: 4.9441\n",
      "Epoch 938/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 1.882 - 0s 361us/sample - loss: 2.5737 - val_loss: 2.9548\n",
      "Epoch 939/1000\n",
      "80/80 [==============================] - 0s 397us/sample - loss: 1.9286 - val_loss: 1.2957\n",
      "Epoch 940/1000\n",
      "80/80 [==============================] - 0s 348us/sample - loss: 2.3650 - val_loss: 4.3191\n",
      "Epoch 941/1000\n",
      "80/80 [==============================] - 0s 383us/sample - loss: 2.3595 - val_loss: 3.6028\n",
      "Epoch 942/1000\n",
      "80/80 [==============================] - 0s 383us/sample - loss: 2.2869 - val_loss: 1.5199\n",
      "Epoch 943/1000\n",
      "80/80 [==============================] - 0s 320us/sample - loss: 2.2860 - val_loss: 2.8704\n",
      "Epoch 944/1000\n",
      "80/80 [==============================] - 0s 373us/sample - loss: 2.8920 - val_loss: 3.3128\n",
      "Epoch 945/1000\n",
      "80/80 [==============================] - 0s 399us/sample - loss: 2.5853 - val_loss: 2.6653\n",
      "Epoch 946/1000\n",
      "80/80 [==============================] - 0s 352us/sample - loss: 2.2403 - val_loss: 3.4692\n",
      "Epoch 947/1000\n",
      "80/80 [==============================] - 0s 567us/sample - loss: 2.3368 - val_loss: 2.0101\n",
      "Epoch 948/1000\n",
      "80/80 [==============================] - 0s 381us/sample - loss: 2.3378 - val_loss: 4.4201\n",
      "Epoch 949/1000\n",
      "80/80 [==============================] - 0s 366us/sample - loss: 2.2775 - val_loss: 2.2119\n",
      "Epoch 950/1000\n",
      "80/80 [==============================] - 0s 501us/sample - loss: 2.1238 - val_loss: 1.7203\n",
      "Epoch 951/1000\n",
      "80/80 [==============================] - 0s 516us/sample - loss: 2.2183 - val_loss: 3.5645\n",
      "Epoch 952/1000\n",
      "80/80 [==============================] - 0s 563us/sample - loss: 2.4221 - val_loss: 2.0497\n",
      "Epoch 953/1000\n",
      "80/80 [==============================] - 0s 423us/sample - loss: 2.1866 - val_loss: 3.1121\n",
      "Epoch 954/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 1.890 - 0s 372us/sample - loss: 2.1163 - val_loss: 3.5924\n",
      "Epoch 955/1000\n",
      "80/80 [==============================] - 0s 307us/sample - loss: 2.3064 - val_loss: 1.9857\n",
      "Epoch 956/1000\n",
      "80/80 [==============================] - 0s 415us/sample - loss: 1.9057 - val_loss: 3.1652\n",
      "Epoch 957/1000\n",
      "80/80 [==============================] - 0s 408us/sample - loss: 2.0337 - val_loss: 2.4473\n",
      "Epoch 958/1000\n",
      "80/80 [==============================] - 0s 293us/sample - loss: 1.9630 - val_loss: 3.2614\n",
      "Epoch 959/1000\n",
      "80/80 [==============================] - 0s 495us/sample - loss: 1.8473 - val_loss: 2.3399\n",
      "Epoch 960/1000\n",
      "80/80 [==============================] - 0s 363us/sample - loss: 1.9679 - val_loss: 2.7425\n",
      "Epoch 961/1000\n",
      "80/80 [==============================] - 0s 276us/sample - loss: 1.8681 - val_loss: 4.1668\n",
      "Epoch 962/1000\n",
      "80/80 [==============================] - 0s 443us/sample - loss: 2.1514 - val_loss: 2.7767\n",
      "Epoch 963/1000\n",
      "80/80 [==============================] - 0s 314us/sample - loss: 1.7728 - val_loss: 3.3394\n",
      "Epoch 964/1000\n",
      "80/80 [==============================] - 0s 396us/sample - loss: 2.3825 - val_loss: 2.7489\n",
      "Epoch 965/1000\n",
      "80/80 [==============================] - 0s 373us/sample - loss: 1.9699 - val_loss: 4.7864\n",
      "Epoch 966/1000\n",
      "80/80 [==============================] - 0s 430us/sample - loss: 2.2527 - val_loss: 3.1414\n",
      "Epoch 967/1000\n",
      "80/80 [==============================] - 0s 378us/sample - loss: 2.2953 - val_loss: 1.3280\n",
      "Epoch 968/1000\n",
      "80/80 [==============================] - 0s 341us/sample - loss: 2.1717 - val_loss: 4.1945\n",
      "Epoch 969/1000\n",
      "80/80 [==============================] - 0s 277us/sample - loss: 2.3296 - val_loss: 4.1484\n",
      "Epoch 970/1000\n",
      "80/80 [==============================] - 0s 369us/sample - loss: 1.9208 - val_loss: 1.4257\n",
      "Epoch 971/1000\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 2.2039 - val_loss: 2.3613\n",
      "Epoch 972/1000\n",
      "80/80 [==============================] - 0s 340us/sample - loss: 1.8584 - val_loss: 3.6179\n",
      "Epoch 973/1000\n",
      "80/80 [==============================] - 0s 261us/sample - loss: 1.9846 - val_loss: 3.2520\n",
      "Epoch 974/1000\n",
      "80/80 [==============================] - 0s 255us/sample - loss: 1.8886 - val_loss: 1.7411\n",
      "Epoch 975/1000\n",
      "80/80 [==============================] - 0s 403us/sample - loss: 2.3381 - val_loss: 2.4420\n",
      "Epoch 976/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 1.9171 - val_loss: 5.2054\n",
      "Epoch 977/1000\n",
      "80/80 [==============================] - 0s 394us/sample - loss: 2.2094 - val_loss: 2.3235\n",
      "Epoch 978/1000\n",
      "80/80 [==============================] - 0s 363us/sample - loss: 2.3376 - val_loss: 1.4866\n",
      "Epoch 979/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.897 - 0s 368us/sample - loss: 2.0590 - val_loss: 5.1420\n",
      "Epoch 980/1000\n",
      "80/80 [==============================] - 0s 327us/sample - loss: 2.5413 - val_loss: 2.2504\n",
      "Epoch 981/1000\n",
      "80/80 [==============================] - 0s 295us/sample - loss: 1.8936 - val_loss: 2.4499\n",
      "Epoch 982/1000\n",
      "80/80 [==============================] - 0s 439us/sample - loss: 1.9769 - val_loss: 3.9362\n",
      "Epoch 983/1000\n",
      "80/80 [==============================] - 0s 593us/sample - loss: 2.2532 - val_loss: 2.9893\n",
      "Epoch 984/1000\n",
      "80/80 [==============================] - 0s 317us/sample - loss: 1.8897 - val_loss: 2.5236\n",
      "Epoch 985/1000\n",
      "80/80 [==============================] - 0s 336us/sample - loss: 2.0678 - val_loss: 3.5117\n",
      "Epoch 986/1000\n",
      "80/80 [==============================] - 0s 380us/sample - loss: 2.0433 - val_loss: 3.2580\n",
      "Epoch 987/1000\n",
      "80/80 [==============================] - 0s 258us/sample - loss: 1.9740 - val_loss: 3.4922\n",
      "Epoch 988/1000\n",
      "80/80 [==============================] - 0s 362us/sample - loss: 2.2290 - val_loss: 3.5672\n",
      "Epoch 989/1000\n",
      "80/80 [==============================] - 0s 366us/sample - loss: 1.9375 - val_loss: 2.0630\n",
      "Epoch 990/1000\n",
      "80/80 [==============================] - 0s 407us/sample - loss: 1.9169 - val_loss: 2.2699\n",
      "Epoch 991/1000\n",
      "80/80 [==============================] - 0s 362us/sample - loss: 2.0848 - val_loss: 3.9353\n",
      "Epoch 992/1000\n",
      "80/80 [==============================] - 0s 378us/sample - loss: 2.1104 - val_loss: 3.0556\n",
      "Epoch 993/1000\n",
      "80/80 [==============================] - 0s 358us/sample - loss: 2.1814 - val_loss: 1.5295\n",
      "Epoch 994/1000\n",
      "80/80 [==============================] - 0s 483us/sample - loss: 2.0516 - val_loss: 3.5165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 995/1000\n",
      "80/80 [==============================] - 0s 353us/sample - loss: 1.9178 - val_loss: 3.3180\n",
      "Epoch 996/1000\n",
      "80/80 [==============================] - 0s 332us/sample - loss: 2.0999 - val_loss: 2.6574\n",
      "Epoch 997/1000\n",
      "80/80 [==============================] - ETA: 0s - loss: 1.200 - 0s 487us/sample - loss: 1.7752 - val_loss: 2.6631\n",
      "Epoch 998/1000\n",
      "80/80 [==============================] - 0s 486us/sample - loss: 2.1799 - val_loss: 2.8260\n",
      "Epoch 999/1000\n",
      "80/80 [==============================] - 0s 519us/sample - loss: 2.0542 - val_loss: 2.7799\n",
      "Epoch 1000/1000\n",
      "80/80 [==============================] - 0s 483us/sample - loss: 2.0196 - val_loss: 2.5324\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Training configuration\n",
    "#\n",
    "ValidationSplit = 0.2\n",
    "BatchSize       = 32\n",
    "Nepochs         = 1000\n",
    "#DropoutValue    = 0.2\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(300, input_dim=1, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
    "  tf.keras.layers.Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
    "  tf.keras.layers.Dense(64, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
    "  #tf.keras.layers.Dropout(DropoutValue),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\033[92mWill train a multilayer perceptron using some toy data following y = x^2\\033[0m\")\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "print(\"Input data MNIST\")\n",
    "#print(\"2 layer MLP with configuration 1:128:128:1\")\n",
    "#print(\"Dropout values       = \", DropoutValue)\n",
    "print(\"Leaky relu parameter =  0.1\")\n",
    "print(\"ValidationSplit      = \", ValidationSplit)\n",
    "print(\"BatchSize            = \", BatchSize)\n",
    "print(\"Nepochs              = \", Nepochs)\n",
    "\n",
    "# now specify the loss function \n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# now we can train the model to make predictions.\n",
    "#   Use the ADAM optimiser\n",
    "#   Specify the metrics to report as accuracy\n",
    "#   Specify the loss function (see above)\n",
    "# the fit step specifies the number of training epochs\n",
    "model.compile(optimizer='adam', loss=loss_fn)\n",
    "history  = model.fit(x_train, y_train, validation_split=ValidationSplit, batch_size=BatchSize, epochs=Nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDisplay the evolution of the loss as a function of the training epoch\u001b[0m\n",
      "\n",
      "  N(Epochs)        =  1000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlEXBISIiZfn/DrT/ypYjD10AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1 - 0s - loss: 1.6683\n",
      "loss = 2.386\n",
      "\n",
      "\u001b[1mDisplay the model prediction against the ground truth from test data\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mDisplay the difference between the model prediction and the ground truth from test data\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEWCAYAAACe8xtsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\033[1mDisplay the evolution of the loss as a function of the training epoch\\033[0m\\n\")\n",
    "print(\"  N(Epochs)        = \", Nepochs)\n",
    "#print(\"  loss (train)     = \", history.history['loss'])\n",
    "#print(\"  loss (test)      = \", history.history['val_loss'])\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validate'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# having finished training the model, use this to evaluate the performance on an\n",
    "# independent sample of test data\n",
    "loss = model.evaluate(x_test,  y_test, verbose=2)\n",
    "print(\"loss = {:5.3f}\".format(loss))\n",
    "\n",
    "print(\"\\n\\033[1mDisplay the model prediction against the ground truth from test data\\033[0m\\n\")\n",
    "\n",
    "#\n",
    "# use the model to make predictions based on the unseen test data\n",
    "#\n",
    "y_predict = model.predict(x_test)\n",
    "plt.plot(x_test, y_predict, \"r.\")\n",
    "plt.plot(x_test, y_test, \"b.\")\n",
    "plt.title('model prediction')\n",
    "plt.ylabel('$y=x^{2}$')\n",
    "plt.xlabel('$x$')\n",
    "plt.legend(['model', 'ground truth'], loc='lower right')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "print(\"\\n\\033[1mDisplay the difference between the model prediction and the ground truth from test data\\033[0m\\n\")\n",
    "\n",
    "delta = []\n",
    "deltapc = []\n",
    "for i in range(len(y_predict)):\n",
    "    thedelta = y_predict[i]-y_test[i]\n",
    "    delta.append( thedelta )\n",
    "    if( x_test[i] ):\n",
    "       deltapc.append( thedelta /  x_test[i] )\n",
    "    else:\n",
    "       deltapc.append( 0.0 )\n",
    "    \n",
    "plt.plot(x_test, delta, \"b.\")\n",
    "plt.plot(x_test, deltapc, \"r.\")\n",
    "plt.legend(['$\\Delta_y$', '$\\Delta_y$ (fraction)'], loc='upper right')\n",
    "plt.title('model prediction accuracy')\n",
    "plt.ylabel('$\\widehat{y}-y$')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylim(-20, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
